{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ HYBRID PROPHET + LSTM AUTOMATED PIPELINE\n",
    "## Multi-Resolution Hybrid Forecasting with Auto-Benchmark\n",
    "\n",
    "### üöÄ AUTO-RUN CONFIGURATION:\n",
    "**Just click \"Run All\" and the notebook will automatically:**\n",
    "1. Train Hybrid models for ALL resolutions (1min, 5min, 15min)\n",
    "2. Forecast BOTH targets (request_count, total_bytes)\n",
    "3. Combine Prophet (trend + seasonality) with LSTM (residual patterns)\n",
    "4. Detect anomalies using dynamic thresholds\n",
    "5. Generate comprehensive benchmarks\n",
    "6. Compare Hybrid vs Pure Prophet vs Pure LSTM\n",
    "\n",
    "### üìä Total Configurations:\n",
    "- **3 resolutions** √ó **2 targets** = **6 hybrid models**\n",
    "- Each configuration trains 3 models: Prophet, LSTM, Hybrid\n",
    "- Expected runtime: **40-70 minutes** (with GPU)\n",
    "\n",
    "### üß¨ Hybrid Architecture:\n",
    "```\n",
    "Prophet (Trend + Seasonality)\n",
    "    ‚Üì\n",
    "Residual = Actual - Prophet_Forecast\n",
    "    ‚Üì\n",
    "LSTM (Learn Residual Patterns)\n",
    "    ‚Üì\n",
    "Final = Prophet_Forecast + LSTM_Residual\n",
    "```\n",
    "\n",
    "### üìÅ Output Structure:\n",
    "```\n",
    "RESULTS_HYBRID/\n",
    "‚îú‚îÄ‚îÄ 1min_request_count/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ prophet_model/ (forecast, components)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ lstm_model.keras\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ scaler.pkl\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ hybrid_predictions.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ anomalies.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ metrics_comparison.csv (Prophet vs LSTM vs Hybrid)\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ visualizations.png\n",
    "‚îú‚îÄ‚îÄ ... (5 more configurations)\n",
    "‚îî‚îÄ‚îÄ FINAL_BENCHMARK/\n",
    "    ‚îú‚îÄ‚îÄ comprehensive_comparison.csv\n",
    "    ‚îú‚îÄ‚îÄ hybrid_vs_individual.csv\n",
    "    ‚îú‚îÄ‚îÄ final_report.txt\n",
    "    ‚îî‚îÄ‚îÄ benchmark_visualizations.png\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:13:59.939037Z",
     "iopub.status.busy": "2026-02-02T13:13:59.938423Z",
     "iopub.status.idle": "2026-02-02T13:14:03.443403Z",
     "shell.execute_reply": "2026-02-02T13:14:03.442638Z",
     "shell.execute_reply.started": "2026-02-02T13:13:59.939013Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYBRID PROPHET + LSTM AUTOMATED TRAINING PIPELINE\n",
      "======================================================================\n",
      "  TensorFlow version: 2.19.0\n",
      "  GPU available: True\n",
      "  Start time: 2026-02-02 13:14:03\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# CELL 2: SETUP & INSTALLATIONS\n",
    "# ===========================\n",
    "\n",
    "!pip install prophet tensorflow scikit-learn -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.layers import Attention, Input, Concatenate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Prophet\n",
    "from prophet import Prophet\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (18, 6)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HYBRID PROPHET + LSTM AUTOMATED TRAINING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  TensorFlow version: {tf.__version__}\")\n",
    "print(f\"  GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(f\"  Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:14:03.445228Z",
     "iopub.status.busy": "2026-02-02T13:14:03.445005Z",
     "iopub.status.idle": "2026-02-02T13:14:03.454606Z",
     "shell.execute_reply": "2026-02-02T13:14:03.453937Z",
     "shell.execute_reply.started": "2026-02-02T13:14:03.445206Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã CONFIGURATION LOADED:\n",
      "  Resolutions: ['5min', '15min']\n",
      "  Targets: ['request_count', 'total_bytes']\n",
      "  Total configurations: 4\n",
      "\n",
      "  Data directory: /kaggle/input/nasa-1/PROCESSED_DATAFINAL\n",
      "  Results directory: /kaggle/working/\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# CELL 3: GLOBAL CONFIGURATION\n",
    "# ===========================\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = 'data'\n",
    "RESULTS_BASE_DIR = 'models/results_hybrid'\n",
    "\n",
    "# Create base results directory\n",
    "os.makedirs(RESULTS_BASE_DIR, exist_ok=True)\n",
    "\n",
    "# All configurations to run\n",
    "RESOLUTIONS = ['5min', '15min']\n",
    "TARGETS = ['request_count', 'total_bytes']\n",
    "\n",
    "# Resolution-specific parameters\n",
    "RESOLUTION_PARAMS = {\n",
    "    '5min': {\n",
    "        'window': 10,        \n",
    "        'lstm_units': 50,\n",
    "        'epochs': 50,\n",
    "        'batch_size': 50\n",
    "    },\n",
    "    '15min': {\n",
    "        'window': 5,          \n",
    "        'lstm_units': 50,\n",
    "        'epochs': 50,\n",
    "        'batch_size': 50 \n",
    "    }\n",
    "}\n",
    "\n",
    "# Prophet parameters\n",
    "PROPHET_PARAMS = {\n",
    "    'daily_seasonality': False,\n",
    "    'weekly_seasonality': False,\n",
    "    'yearly_seasonality': False,\n",
    "    'changepoint_prior_scale': 5,\n",
    "    'seasonality_prior_scale': 30,\n",
    "    'seasonality_mode': 'multiplicative'\n",
    "}\n",
    "\n",
    "# Storm/outage holiday\n",
    "STORM_HOLIDAY = pd.DataFrame({\n",
    "    'holiday': 'storm_outage',\n",
    "    'ds': pd.date_range(start='1995-08-01 14:52:01', end='1995-08-03 04:36:13', freq='h'),\n",
    "    'lower_window': 0,\n",
    "    'upper_window': 0,\n",
    "})\n",
    "\n",
    "print(\"\\nüìã CONFIGURATION LOADED:\")\n",
    "print(f\"  Resolutions: {RESOLUTIONS}\")\n",
    "print(f\"  Targets: {TARGETS}\")\n",
    "print(f\"  Total configurations: {len(RESOLUTIONS) * len(TARGETS)}\")\n",
    "print(f\"\\n  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Results directory: {RESULTS_BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:14:03.455966Z",
     "iopub.status.busy": "2026-02-02T13:14:03.455664Z",
     "iopub.status.idle": "2026-02-02T13:14:03.478478Z",
     "shell.execute_reply": "2026-02-02T13:14:03.477832Z",
     "shell.execute_reply.started": "2026-02-02T13:14:03.455935Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# CELL 4: UTILITY FUNCTIONS\n",
    "# ===========================\n",
    "\n",
    "def prepare_prophet_data(df, target_col):\n",
    "    \"\"\"\n",
    "    Prepare data for Prophet.\n",
    "    \"\"\"\n",
    "    prophet_df = pd.DataFrame({\n",
    "        'ds': df.index,\n",
    "        'y': df[target_col]\n",
    "    })\n",
    "    \n",
    "    # Add time-based features\n",
    "    prophet_df['hour'] = prophet_df['ds'].dt.hour\n",
    "    prophet_df['day_of_week'] = prophet_df['ds'].dt.dayofweek\n",
    "    prophet_df['is_weekend'] = (prophet_df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    return prophet_df\n",
    "\n",
    "\n",
    "def make_sequences(data, window):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(data)):\n",
    "        X.append(data[i-window:i])\n",
    "        y.append(data[i])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_multivariate_features(df, train_size, window):\n",
    "    \"\"\"\n",
    "    T·∫°o b·ªô d·ªØ li·ªáu ƒëa chi·ªÅu cho LSTM:\n",
    "    1. Residual (Scaled)\n",
    "    2. Prophet Prediction (Scaled) - ƒê·ªÉ h·ªçc ƒë·ªô l·ªõn\n",
    "    3. Hour Sin/Cos - ƒê·ªÉ h·ªçc t√≠nh chu k·ª≥ th·ªùi gian\n",
    "    \"\"\"\n",
    "    # 1. Feature: Residual\n",
    "    resid_values = df[['residual']].values\n",
    "    scaler_resid = StandardScaler()\n",
    "    # Ch·ªâ fit tr√™n t·∫≠p train\n",
    "    scaler_resid.fit(resid_values[:train_size]) \n",
    "    resid_scaled = scaler_resid.transform(resid_values)\n",
    "    \n",
    "    # 2. Feature: Context (Prophet Prediction)\n",
    "    yhat_values = df[['yhat_prophet']].values\n",
    "    scaler_context = StandardScaler()\n",
    "    scaler_context.fit(yhat_values[:train_size])\n",
    "    yhat_scaled = scaler_context.transform(yhat_values)\n",
    "    \n",
    "    # 3. Feature: Time Cyclical Encoding\n",
    "    # Bi·∫øn ƒë·ªïi gi·ªù th√†nh v√≤ng tr√≤n (23h g·∫ßn 0h)\n",
    "    hour_sin = np.sin(2 * np.pi * df.index.hour / 24).values.reshape(-1, 1)\n",
    "    hour_cos = np.cos(2 * np.pi * df.index.hour / 24).values.reshape(-1, 1)\n",
    "    \n",
    "    # G·ªôp t·∫•t c·∫£ features: Shape (N, 4)\n",
    "    # [Residual, Prophet_Hat, Hour_Sin, Hour_Cos]\n",
    "    features_matrix = np.hstack([resid_scaled, yhat_scaled, hour_sin, hour_cos])\n",
    "    \n",
    "    return features_matrix, scaler_resid\n",
    "\n",
    "def make_multivariate_sequences(features, targets, window):\n",
    "    \"\"\"\n",
    "    X: Multivariate features (t-window ... t-1)\n",
    "    y: Target residual (t)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(window, len(features)):\n",
    "        X.append(features[i-window:i, :]) # L·∫•y c·ª≠a s·ªï c·ªßa T·∫§T C·∫¢ features\n",
    "        y.append(targets[i])             # Ch·ªâ d·ª± ƒëo√°n residual (ƒë√£ scale)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics.\n",
    "    \"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # MAPE (avoid division by zero)\n",
    "    mask = y_true != 0\n",
    "    mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100 if mask.sum() > 0 else 0\n",
    "    \n",
    "    # R¬≤\n",
    "    r2 = 1 - (np.sum((y_true - y_pred)**2) / np.sum((y_true - y_true.mean())**2))\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAPE': mape,\n",
    "        'R2': r2\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def detect_anomalies(y_true, y_pred, window_size, threshold=3):\n",
    "    \"\"\"\n",
    "    Detect anomalies using dynamic Z-score on prediction error.\n",
    "    \"\"\"\n",
    "    error = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    rolling_mean = pd.Series(error).rolling(window_size, min_periods=1).mean()\n",
    "    rolling_std = pd.Series(error).rolling(window_size, min_periods=1).std()\n",
    "    \n",
    "    # Z-score\n",
    "    z_score = (error - rolling_mean) / (rolling_std + 1e-8)\n",
    "    \n",
    "    # Anomaly indices\n",
    "    anomaly_mask = z_score > threshold\n",
    "    anomaly_indices = np.where(anomaly_mask)[0]\n",
    "    \n",
    "    return anomaly_indices, z_score.values\n",
    "\n",
    "\n",
    "print(\"‚úì Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:14:03.480684Z",
     "iopub.status.busy": "2026-02-02T13:14:03.480436Z",
     "iopub.status.idle": "2026-02-02T13:14:03.502436Z",
     "shell.execute_reply": "2026-02-02T13:14:03.501731Z",
     "shell.execute_reply.started": "2026-02-02T13:14:03.480663Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ===========================================================================================\n",
    "# # CELL 5: MAIN HYBRID TRAINING FUNCTION (FINAL VERSION: LOG + STORM MASKING)\n",
    "# # ===========================================================================================\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "# from keras.models import Sequential\n",
    "# from keras.losses import Huber\n",
    "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from prophet import Prophet\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import time\n",
    "# import pickle\n",
    "\n",
    "# def train_hybrid_model(resolution, target, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Train Hybrid Prophet + LSTM model with:\n",
    "#     1. Storm Masking (NaN for outage periods)\n",
    "#     2. Input Log Transformation (Amplitude Correction)\n",
    "#     3. Robust LSTM Architecture (Bidirectional + Huber Loss)\n",
    "#     \"\"\"\n",
    "#     if verbose:\n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(f\"TRAINING: Hybrid (Log + Storm-Masked) | {resolution} | {target}\")\n",
    "#         print(f\"{'='*70}\")\n",
    "    \n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # --- C·∫§U H√åNH ---\n",
    "#     # L·∫•y params t·ª´ dict global ho·∫∑c hardcode n·∫øu c·∫ßn\n",
    "#     params = RESOLUTION_PARAMS[resolution]\n",
    "#     window = params['window']\n",
    "#     lstm_units = params['lstm_units']\n",
    "#     epochs = params['epochs']\n",
    "#     batch_size = params['batch_size']\n",
    "    \n",
    "#     results_dir = f\"{RESULTS_BASE_DIR}/{resolution}_{target}_final\"\n",
    "#     os.makedirs(results_dir, exist_ok=True)\n",
    "#     os.makedirs(f\"{results_dir}/prophet_model\", exist_ok=True)\n",
    "    \n",
    "#     try:\n",
    "#         # ==================\n",
    "#         # 1. LOAD DATA & STORM MASKING\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"[1/7] Loading data & Masking Storm...\")\n",
    "        \n",
    "#         train_df = pd.read_csv(f\"{DATA_DIR}/train_{resolution}.csv\", index_col=0, parse_dates=True)\n",
    "#         test_df = pd.read_csv(f\"{DATA_DIR}/test_{resolution}.csv\", index_col=0, parse_dates=True)\n",
    "        \n",
    "#         # L∆∞u gi√° tr·ªã th·ª±c g·ªëc (Raw scale) ƒë·ªÉ ƒë√°nh gi√° sau c√πng\n",
    "#         y_test_raw_true = test_df[target].values\n",
    "\n",
    "#         # --- [NEW] X·ª¨ L√ù B√ÉO (G√°n NaN) ---\n",
    "#         # B√£o: 14:52:01 01/08/1995 -> 04:36:13 03/08/1995\n",
    "#         # Ch·ªâ x·ª≠ l√Ω tr√™n Train set v√¨ b√£o n·∫±m trong th√°ng 8\n",
    "#         storm_start = pd.Timestamp(\"1995-08-01 14:52:01\")\n",
    "#         storm_end   = pd.Timestamp(\"1995-08-03 04:36:13\")\n",
    "        \n",
    "#         mask = (train_df.index >= storm_start) & (train_df.index <= storm_end)\n",
    "#         train_df.loc[mask, target] = None # G√°n NaN ƒë·ªÉ Prophet b·ªè qua\n",
    "        \n",
    "#         if verbose: print(f\"   -> Masked {mask.sum()} intervals as NaN due to storm.\")\n",
    "\n",
    "#         # --- LOG TRANSFORMATION ---\n",
    "#         # np.log1p x·ª≠ l√Ω t·ªët NaN (v·∫´n gi·ªØ l√† NaN)\n",
    "#         train_df[target] = np.log1p(train_df[target])\n",
    "#         test_df[target] = np.log1p(test_df[target])\n",
    "        \n",
    "#         full_df = pd.concat([train_df, test_df]).sort_index()\n",
    "#         train_size = len(train_df)\n",
    "        \n",
    "#         # ==================\n",
    "#         # 2. TRAIN PROPHET (LOG SPACE)\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[2/7] Training Prophet on Log Data...\")\n",
    "        \n",
    "#         prophet_train = prepare_prophet_data(train_df, target)\n",
    "        \n",
    "#         prophet_model = Prophet(\n",
    "#             changepoint_prior_scale=5.1, \n",
    "#             seasonality_prior_scale=30,\n",
    "#             seasonality_mode='additive', # Log space -> Additive\n",
    "#         )\n",
    "        \n",
    "#         # Seasonality (Kh·ªõp b√†i b√°o)\n",
    "#         prophet_model.add_seasonality(name='daily_high_freq', period=1, fourier_order=50)\n",
    "#         prophet_model.add_seasonality(name='weekly_high_freq', period=7, fourier_order=20)\n",
    "        \n",
    "#         prophet_model.add_regressor('hour')\n",
    "#         prophet_model.add_regressor('day_of_week')\n",
    "#         prophet_model.add_regressor('is_weekend')\n",
    "        \n",
    "#         prophet_model.fit(prophet_train)\n",
    "        \n",
    "#         # Forecast\n",
    "#         prophet_full = prepare_prophet_data(full_df, target)\n",
    "#         prophet_forecast = prophet_model.predict(prophet_full[['ds', 'hour', 'day_of_week', 'is_weekend']])\n",
    "        \n",
    "#         # ==================\n",
    "#         # 3. COMPUTE RESIDUALS\n",
    "#         # ==================\n",
    "#         full_df['yhat_prophet'] = prophet_forecast['yhat'].values\n",
    "#         full_df['residual'] = full_df[target] - full_df['yhat_prophet']\n",
    "        \n",
    "#         # --- [NEW] FILLNA FOR RESIDUALS ---\n",
    "#         # T·∫°i v√πng b√£o, target=NaN n√™n residual=NaN. LSTM kh√¥ng hi·ªÉu NaN.\n",
    "#         # Ta fill 0 (coi nh∆∞ Prophet d·ª± ƒëo√°n ƒë√∫ng xu h∆∞·ªõng, kh√¥ng c√≥ l·ªói d∆∞ th·ª´a)\n",
    "#         full_df['residual'] = full_df['residual'].fillna(0)\n",
    "        \n",
    "#         if verbose: print(f\" Residual std (log-space): {full_df['residual'].std():.3f}\")\n",
    "        \n",
    "#         # ==================\n",
    "#         # 4. PREPARE LSTM DATA\n",
    "#         # ==================\n",
    "#         scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        \n",
    "#         residual_train_values = full_df.iloc[:train_size][['residual']].values\n",
    "#         scaler.fit(residual_train_values)\n",
    "#         residual_train_scaled = scaler.transform(residual_train_values)\n",
    "        \n",
    "#         X_train, y_train = make_sequences(residual_train_scaled.flatten(), window)\n",
    "#         X_train = X_train.reshape(-1, window, 1)\n",
    "        \n",
    "#         # Split Valid\n",
    "#         val_ratio = 0.2\n",
    "#         val_size = int(len(X_train) * val_ratio)\n",
    "#         X_train_sub = X_train[:-val_size]\n",
    "#         y_train_sub = y_train[:-val_size]\n",
    "#         X_val = X_train[-val_size:]\n",
    "#         y_val = y_train[-val_size:]\n",
    "        \n",
    "#         # ==================\n",
    "#         # 5. TRAIN LSTM (ROBUST ARCHITECTURE)\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[5/7] Training Robust LSTM...\")\n",
    "        \n",
    "#         lstm_model = Sequential([\n",
    "#             Bidirectional(LSTM(lstm_units, return_sequences=True, input_shape=(window, 1))),\n",
    "#             Bidirectional(LSTM(lstm_units, return_sequences=False)),\n",
    "#             Dropout(0.2),\n",
    "#             Dense(1)\n",
    "#         ])\n",
    "        \n",
    "#         lstm_model.compile(optimizer='adam', loss=Huber(delta=1.0), metrics=['mae'])\n",
    "        \n",
    "#         callbacks = [\n",
    "#             EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss', verbose=1),\n",
    "#             ReduceLROnPlateau(patience=5, factor=0.5, monitor='val_loss', verbose=1)\n",
    "#         ]\n",
    "        \n",
    "#         history = lstm_model.fit(\n",
    "#             X_train_sub, y_train_sub,\n",
    "#             validation_data=(X_val, y_val),\n",
    "#             epochs=epochs,\n",
    "#             batch_size=batch_size,\n",
    "#             callbacks=callbacks,\n",
    "#             verbose=1\n",
    "#         )\n",
    "        \n",
    "#         # ==================\n",
    "#         # 6. HYBRID PREDICTION\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[6/7] Generating hybrid predictions...\")\n",
    "        \n",
    "#         # Predict Residuals (All data)\n",
    "#         residual_all_values = full_df[['residual']].values\n",
    "#         residual_all_scaled = scaler.transform(residual_all_values)\n",
    "        \n",
    "#         X_all, _ = make_sequences(residual_all_scaled.flatten(), window)\n",
    "#         X_all = X_all.reshape(-1, window, 1)\n",
    "        \n",
    "#         residual_pred_scaled = lstm_model.predict(X_all, verbose=0)\n",
    "#         residual_pred_log = scaler.inverse_transform(residual_pred_scaled).flatten()\n",
    "        \n",
    "#         # Reconstruct Hybrid (Log Space)\n",
    "#         prophet_pred_log = full_df['yhat_prophet'].iloc[window:].values\n",
    "#         hybrid_pred_log = prophet_pred_log + residual_pred_log\n",
    "        \n",
    "#         # Inverse Transform (Log -> Real)\n",
    "#         hybrid_pred_final = np.expm1(hybrid_pred_log)\n",
    "#         prophet_pred_final = np.expm1(prophet_pred_log)\n",
    "        \n",
    "#         # ==================\n",
    "#         # 7. EVALUATION\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[7/7] Evaluating models...\")\n",
    "\n",
    "#         # C·∫Øt d·ªØ li·ªáu Test\n",
    "#         lstm_test_start_idx = train_size - window\n",
    "        \n",
    "#         hybrid_test_pred = hybrid_pred_final[lstm_test_start_idx:]\n",
    "#         prophet_test_pred = prophet_pred_final[lstm_test_start_idx:]\n",
    "#         residual_test_pred = residual_pred_log[lstm_test_start_idx:] \n",
    "\n",
    "#         # L·∫•y d·ªØ li·ªáu th·ª±c t·∫ø (L·∫•y t·ª´ Log Residuals ƒë·ªÉ so s√°nh LSTM)\n",
    "#         residual_test_true = full_df['residual'].iloc[train_size:].values\n",
    "        \n",
    "#         # ƒê·ªìng b·ªô ƒë·ªô d√†i\n",
    "#         min_len = min(len(y_test_raw_true), len(hybrid_test_pred), len(residual_test_true))\n",
    "        \n",
    "#         y_test_true = y_test_raw_true[:min_len]\n",
    "#         hybrid_test_pred = hybrid_test_pred[:min_len]\n",
    "#         prophet_test_pred = prophet_test_pred[:min_len]\n",
    "#         residual_test_true = residual_test_true[:min_len]\n",
    "#         residual_test_pred = residual_test_pred[:min_len]\n",
    "        \n",
    "#         # Metrics\n",
    "#         prophet_metrics = calculate_metrics(y_test_true, prophet_test_pred, \"Prophet\")\n",
    "#         hybrid_metrics = calculate_metrics(y_test_true, hybrid_test_pred, \"Hybrid\")\n",
    "#         lstm_metrics = calculate_metrics(residual_test_true, residual_test_pred, \"LSTM_Residuals\")\n",
    "        \n",
    "#         residual_mape = np.mean(\n",
    "#             np.abs((residual_test_true - residual_test_pred) / (np.abs(residual_test_true) + 1e-8))\n",
    "#         ) * 100\n",
    "        \n",
    "#         if verbose:\n",
    "#             print(f\"\\n>>> LSTM Residuals MAPE on test: {residual_mape:.2f}% <<<\")\n",
    "#             print(\n",
    "#                 f\" Prophet - MAE: {prophet_metrics['MAE']:.2f}, \"\n",
    "#                 f\"MSE: {prophet_metrics['MSE']:.2f}, \"\n",
    "#                 f\"MAPE: {prophet_metrics['MAPE']:.2f}%, \"\n",
    "#                 f\"RMSE: {prophet_metrics['RMSE']:.2f}\"\n",
    "#             )\n",
    "#             # print(\n",
    "#             #     f\" LSTM Residuals - MAE: {lstm_metrics['MAE']:.2f}, \"\n",
    "#             #     f\"MSE: {lstm_metrics['MSE']:.2f}, \"\n",
    "#             #     f\"MAPE: {lstm_metrics['MAPE']:.2f}%, \"\n",
    "#             #     f\"RMSE: {lstm_metrics['RMSE']:.2f}\"\n",
    "#             # )\n",
    "#             print(\n",
    "#                 f\" Hybrid - MAE: {hybrid_metrics['MAE']:.2f}, \"\n",
    "#                 f\"MSE: {hybrid_metrics['MSE']:.2f}, \"\n",
    "#                 f\"MAPE: {hybrid_metrics['MAPE']:.2f}%, \"\n",
    "#                 f\"RMSE: {hybrid_metrics['RMSE']:.2f}\"\n",
    "#             )\n",
    "        \n",
    "#         # Save results (Gi·ªØ nguy√™n code save c·ªßa b·∫°n)\n",
    "#         elapsed_time = time.time() - start_time\n",
    "#         return {\n",
    "#             'resolution': resolution,\n",
    "#             'target': target,\n",
    "#             'prophet_mae': prophet_metrics['MAE'],\n",
    "#             'prophet_rmse': prophet_metrics['RMSE'],\n",
    "#             'prophet_mse': prophet_metrics['MSE'],\n",
    "#             'prophet_mape': prophet_metrics['MAPE'],\n",
    "#             'lstm_mae': lstm_metrics['MAE'],\n",
    "#             'lstm_rmse': lstm_metrics['RMSE'],\n",
    "#             'lstm_mse': lstm_metrics['MSE'],\n",
    "#             'lstm_mape': lstm_metrics['MAPE'],\n",
    "#             'hybrid_mae': hybrid_metrics['MAE'],\n",
    "#             'hybrid_rmse': hybrid_metrics['RMSE'],\n",
    "#             'hybrid_mse': hybrid_metrics['MSE'],\n",
    "#             'hybrid_mape': hybrid_metrics['MAPE'],\n",
    "#             'hybrid_r2': hybrid_metrics['R2'],\n",
    "#             'residual_mape_test': residual_mape,\n",
    "#             'training_time_sec': elapsed_time,\n",
    "#             'results_dir': results_dir\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:14:03.504000Z",
     "iopub.status.busy": "2026-02-02T13:14:03.503753Z",
     "iopub.status.idle": "2026-02-02T13:14:03.523077Z",
     "shell.execute_reply": "2026-02-02T13:14:03.522373Z",
     "shell.execute_reply.started": "2026-02-02T13:14:03.503981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ===========================================================================================\n",
    "# # CELL 5: MAIN HYBRID TRAINING FUNCTION (NO LOG TRANSFORM VERSION)\n",
    "# # ===========================================================================================\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "# from keras.models import Sequential\n",
    "# from keras.losses import Huber\n",
    "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from prophet import Prophet\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import time\n",
    "# import pickle\n",
    "\n",
    "# def train_hybrid_model(resolution, target, verbose=True):\n",
    "#     \"\"\"\n",
    "#     Train Hybrid Prophet + LSTM model WITHOUT Log Transformation.\n",
    "#     Running on RAW DATA.\n",
    "#     \"\"\"\n",
    "#     if verbose:\n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(f\"TRAINING: Hybrid (RAW DATA + Storm-Masked) | {resolution} | {target}\")\n",
    "#         print(f\"{'='*70}\")\n",
    "    \n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # --- C·∫§U H√åNH ---\n",
    "#     # L·∫•y params t·ª´ dict global ho·∫∑c hardcode\n",
    "#     try:\n",
    "#         params = RESOLUTION_PARAMS[resolution]\n",
    "#     except:\n",
    "#         # Fallback n·∫øu ch∆∞a define dict\n",
    "#         params = {'window': 10, 'lstm_units': 50, 'epochs': 50, 'batch_size': 16}\n",
    "        \n",
    "#     window = params['window']\n",
    "#     lstm_units = params['lstm_units']\n",
    "#     epochs = params['epochs']\n",
    "#     batch_size = params['batch_size']\n",
    "    \n",
    "#     results_dir = f\"{RESULTS_BASE_DIR}/{resolution}_{target}_raw_no_log\"\n",
    "#     os.makedirs(results_dir, exist_ok=True)\n",
    "#     os.makedirs(f\"{results_dir}/prophet_model\", exist_ok=True)\n",
    "    \n",
    "#     try:\n",
    "#         # ==================\n",
    "#         # 1. LOAD DATA & STORM MASKING\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"[1/7] Loading data & Masking Storm (Raw Data)...\")\n",
    "        \n",
    "#         train_df = pd.read_csv(f\"{DATA_DIR}/train_{resolution}.csv\", index_col=0, parse_dates=True)\n",
    "#         test_df = pd.read_csv(f\"{DATA_DIR}/test_{resolution}.csv\", index_col=0, parse_dates=True)\n",
    "        \n",
    "#         # L∆∞u gi√° tr·ªã th·ª±c g·ªëc ƒë·ªÉ ƒë√°nh gi√°\n",
    "#         y_test_raw_true = test_df[target].values\n",
    "\n",
    "#         # --- X·ª¨ L√ù B√ÉO (G√°n NaN) ---\n",
    "#         storm_start = pd.Timestamp(\"1995-08-01 14:52:01\")\n",
    "#         storm_end   = pd.Timestamp(\"1995-08-03 04:36:13\")\n",
    "        \n",
    "#         mask = (train_df.index >= storm_start) & (train_df.index <= storm_end)\n",
    "#         train_df.loc[mask, target] = None # G√°n NaN\n",
    "        \n",
    "#         if verbose: print(f\"   -> Masked {mask.sum()} intervals as NaN due to storm.\")\n",
    "\n",
    "#         # --- [REMOVED] LOG TRANSFORMATION ---\n",
    "        \n",
    "#         full_df = pd.concat([train_df, test_df]).sort_index()\n",
    "#         train_size = len(train_df)\n",
    "        \n",
    "#         # ==================\n",
    "#         # 2. TRAIN PROPHET (RAW SPACE)\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[2/7] Training Prophet on Raw Data...\")\n",
    "        \n",
    "#         prophet_train = prepare_prophet_data(train_df, target)\n",
    "        \n",
    "#         prophet_model = Prophet(\n",
    "#             changepoint_prior_scale=5, \n",
    "#             seasonality_prior_scale=30,\n",
    "#             # L∆∞u √Ω: V·ªõi d·ªØ li·ªáu Raw bi·∫øn ƒë·ªông m·∫°nh, 'multiplicative' th∆∞·ªùng t·ªët h∆°n 'additive'.\n",
    "#             # Tuy nhi√™n ƒë·ªÉ ch·∫°y an to√†n v·ªõi s·ªë 0, ta gi·ªØ 'additive' ho·∫∑c c·∫ßn x·ª≠ l√Ω s·ªë 0 n·∫øu d√πng 'multiplicative'.\n",
    "#             # ·ªû ƒë√¢y gi·ªØ nguy√™n 'additive' ƒë·ªÉ so s√°nh c√¥ng b·∫±ng v·ªõi b·∫£n Log.\n",
    "#             seasonality_mode='multiplicative', \n",
    "#         )\n",
    "        \n",
    "#         # Seasonality\n",
    "#         prophet_model.add_seasonality(name='daily_high_freq', period=1, fourier_order=50)\n",
    "#         prophet_model.add_seasonality(name='weekly_high_freq', period=7, fourier_order=20)\n",
    "        \n",
    "#         prophet_model.add_regressor('hour')\n",
    "#         prophet_model.add_regressor('day_of_week')\n",
    "#         prophet_model.add_regressor('is_weekend')\n",
    "        \n",
    "#         prophet_model.fit(prophet_train)\n",
    "        \n",
    "#         # Forecast\n",
    "#         prophet_full = prepare_prophet_data(full_df, target)\n",
    "#         prophet_forecast = prophet_model.predict(prophet_full[['ds', 'hour', 'day_of_week', 'is_weekend']])\n",
    "        \n",
    "#         # ==================\n",
    "#         # 3. COMPUTE RESIDUALS (RAW SPACE)\n",
    "#         # ==================\n",
    "#         full_df['yhat_prophet'] = prophet_forecast['yhat'].values\n",
    "        \n",
    "#         # Residual = Actual - Predicted (Kh√¥ng ph·∫£i Log - Log n·ªØa)\n",
    "#         full_df['residual'] = full_df[target] - full_df['yhat_prophet']\n",
    "        \n",
    "#         # Fill NaN residuals b·∫±ng 0 (cho v√πng b√£o)\n",
    "#         full_df['residual'] = full_df['residual'].fillna(0)\n",
    "        \n",
    "#         if verbose: \n",
    "#             print(f\" Residual std (raw-space): {full_df['residual'].std():.3f}\")\n",
    "#             # In ra max residual ƒë·ªÉ th·∫•y s·ª± kh√°c bi·ªát v·ªõi b·∫£n Log\n",
    "#             print(f\" Max Residual: {full_df['residual'].max():.3f}\")\n",
    "        \n",
    "#         # ==================\n",
    "#         # 4. PREPARE LSTM DATA\n",
    "#         # ==================\n",
    "#         # MinMaxScaler c·ª±c k·ª≥ quan tr·ªçng ·ªü ƒë√¢y v√¨ bi√™n ƒë·ªô d·ªØ li·ªáu Raw r·∫•t l·ªõn\n",
    "#         scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        \n",
    "#         residual_train_values = full_df.iloc[:train_size][['residual']].values\n",
    "#         scaler.fit(residual_train_values)\n",
    "#         residual_train_scaled = scaler.transform(residual_train_values)\n",
    "        \n",
    "#         X_train, y_train = make_sequences(residual_train_scaled.flatten(), window)\n",
    "#         X_train = X_train.reshape(-1, window, 1)\n",
    "        \n",
    "#         # Split Valid\n",
    "#         val_ratio = 0.1\n",
    "#         val_size = int(len(X_train) * val_ratio)\n",
    "#         X_train_sub = X_train[:-val_size]\n",
    "#         y_train_sub = y_train[:-val_size]\n",
    "#         X_val = X_train[-val_size:]\n",
    "#         y_val = y_train[-val_size:]\n",
    "        \n",
    "#         # ==================\n",
    "#         # 5. TRAIN LSTM\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[5/7] Training Robust LSTM...\")\n",
    "        \n",
    "#         lstm_model = Sequential([\n",
    "#             Bidirectional(LSTM(lstm_units, return_sequences=True, input_shape=(window, 1))),\n",
    "#             Bidirectional(LSTM(lstm_units - 30, return_sequences=False)),\n",
    "#             Dropout(0.2),\n",
    "#             Dense(1)\n",
    "#         ])\n",
    "        \n",
    "#         lstm_model.compile(optimizer='adam', loss=Huber(delta=1.0), metrics=['mae'])\n",
    "        \n",
    "#         callbacks = [\n",
    "#             EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss', verbose=1),\n",
    "#             ReduceLROnPlateau(patience=5, factor=0.5, monitor='val_loss', verbose=1)\n",
    "#         ]\n",
    "        \n",
    "#         history = lstm_model.fit(\n",
    "#             X_train_sub, y_train_sub,\n",
    "#             validation_data=(X_val, y_val),\n",
    "#             epochs=epochs,\n",
    "#             batch_size=batch_size,\n",
    "#             callbacks=callbacks,\n",
    "#             verbose=1\n",
    "#         )\n",
    "        \n",
    "#         # ==================\n",
    "#         # 6. HYBRID PREDICTION\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[6/7] Generating hybrid predictions...\")\n",
    "        \n",
    "#         # Predict Residuals (All data)\n",
    "#         residual_all_values = full_df[['residual']].values\n",
    "#         residual_all_scaled = scaler.transform(residual_all_values)\n",
    "        \n",
    "#         X_all, _ = make_sequences(residual_all_scaled.flatten(), window)\n",
    "#         X_all = X_all.reshape(-1, window, 1)\n",
    "        \n",
    "#         residual_pred_scaled = lstm_model.predict(X_all, verbose=0)\n",
    "#         # ƒê√¢y l√† Residual d·ª± b√°o ·ªü d·∫°ng RAW (ƒë√£ inverse scale)\n",
    "#         residual_pred_raw = scaler.inverse_transform(residual_pred_scaled).flatten()\n",
    "        \n",
    "#         # Reconstruct Hybrid (Raw Space)\n",
    "#         # Hybrid = Prophet + Residual\n",
    "#         prophet_pred_raw = full_df['yhat_prophet'].iloc[window:].values\n",
    "#         hybrid_pred_raw = prophet_pred_raw + residual_pred_raw\n",
    "        \n",
    "#         # --- [REMOVED] INVERSE TRANSFORM ---\n",
    "#         # Kh√¥ng c·∫ßn np.expm1 v√¨ d·ªØ li·ªáu ƒë√£ l√† raw\n",
    "#         hybrid_pred_final = hybrid_pred_raw\n",
    "#         prophet_pred_final = prophet_pred_raw\n",
    "        \n",
    "#         # ==================\n",
    "#         # 7. EVALUATION\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[7/7] Evaluating models...\")\n",
    "\n",
    "#         # C·∫Øt d·ªØ li·ªáu Test\n",
    "#         lstm_test_start_idx = train_size - window\n",
    "        \n",
    "#         hybrid_test_pred = hybrid_pred_final[lstm_test_start_idx:]\n",
    "#         prophet_test_pred = prophet_pred_final[lstm_test_start_idx:]\n",
    "#         residual_test_pred = residual_pred_raw[lstm_test_start_idx:] \n",
    "\n",
    "#         # L·∫•y Residual th·ª±c t·∫ø (Raw Space)\n",
    "#         residual_test_true = full_df['residual'].iloc[train_size:].values\n",
    "        \n",
    "#         # ƒê·ªìng b·ªô ƒë·ªô d√†i\n",
    "#         min_len = min(len(y_test_raw_true), len(hybrid_test_pred), len(residual_test_true))\n",
    "        \n",
    "#         y_test_true = y_test_raw_true[:min_len]\n",
    "#         hybrid_test_pred = hybrid_test_pred[:min_len]\n",
    "#         prophet_test_pred = prophet_test_pred[:min_len]\n",
    "#         residual_test_true = residual_test_true[:min_len]\n",
    "#         residual_test_pred = residual_test_pred[:min_len]\n",
    "        \n",
    "#         # Metrics\n",
    "#         prophet_metrics = calculate_metrics(y_test_true, prophet_test_pred, \"Prophet\")\n",
    "#         hybrid_metrics = calculate_metrics(y_test_true, hybrid_test_pred, \"Hybrid\")\n",
    "        \n",
    "#         # Metrics cho Residuals (Raw Space)\n",
    "#         lstm_metrics = calculate_metrics(residual_test_true, residual_test_pred, \"LSTM_Residuals\")\n",
    "        \n",
    "#         residual_mape = np.mean(\n",
    "#             np.abs((residual_test_true - residual_test_pred) / (np.abs(residual_test_true) + 1e-8))\n",
    "#         ) * 100\n",
    "        \n",
    "#         if verbose:\n",
    "#             print(f\"\\n>>> RESULT (Resolution: {resolution}) - NO LOG TRANSFORM <<<\")\n",
    "#             print(\"-\" * 60)\n",
    "#             print(f\" LSTM Residuals - MAPE: {residual_mape:.2f}% (Prediction of Raw Errors)\")\n",
    "#             print(\"-\" * 60)\n",
    "#             print(\n",
    "#                 f\" Prophet - MAE: {prophet_metrics['MAE']:.2f}, \"\n",
    "#                 f\"MSE: {prophet_metrics['MSE']:.2f}, \"\n",
    "#                 f\"MAPE: {prophet_metrics['MAPE']:.2f}%, \"\n",
    "#                 f\"RMSE: {prophet_metrics['RMSE']:.2f}\"\n",
    "#             )\n",
    "#             print(\n",
    "#                 f\" Hybrid  - MAE: {hybrid_metrics['MAE']:.2f}, \"\n",
    "#                 f\"MSE: {hybrid_metrics['MSE']:.2f}, \"\n",
    "#                 f\"MAPE: {hybrid_metrics['MAPE']:.2f}%, \"\n",
    "#                 f\"RMSE: {hybrid_metrics['RMSE']:.2f}\"\n",
    "#             )\n",
    "        \n",
    "#         # Save results\n",
    "#         elapsed_time = time.time() - start_time\n",
    "#         return {\n",
    "#             'resolution': resolution,\n",
    "#             'target': target,\n",
    "#             'prophet_mae': prophet_metrics['MAE'],\n",
    "#             'prophet_rmse': prophet_metrics['RMSE'],\n",
    "#             'prophet_mse': prophet_metrics['MSE'],\n",
    "#             'prophet_mape': prophet_metrics['MAPE'],\n",
    "#             'lstm_mae': lstm_metrics['MAE'],\n",
    "#             'lstm_rmse': lstm_metrics['RMSE'],\n",
    "#             'lstm_mse': lstm_metrics['MSE'],\n",
    "#             'lstm_mape': lstm_metrics['MAPE'],\n",
    "#             'hybrid_mae': hybrid_metrics['MAE'],\n",
    "#             'hybrid_rmse': hybrid_metrics['RMSE'],\n",
    "#             'hybrid_mse': hybrid_metrics['MSE'],\n",
    "#             'hybrid_mape': hybrid_metrics['MAPE'],\n",
    "#             'hybrid_r2': hybrid_metrics['R2'],\n",
    "#             'residual_mape_test': residual_mape,\n",
    "#             'training_time_sec': elapsed_time,\n",
    "#             'results_dir': results_dir\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:14:03.524827Z",
     "iopub.status.busy": "2026-02-02T13:14:03.524381Z",
     "iopub.status.idle": "2026-02-02T13:14:03.540076Z",
     "shell.execute_reply": "2026-02-02T13:14:03.539595Z",
     "shell.execute_reply.started": "2026-02-02T13:14:03.524805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ===========================================================================================\n",
    "# # CELL 5: MAIN HYBRID TRAINING FUNCTION (UPGRADE: SETUP_1 ARCHITECTURE + STORM MASKING)\n",
    "# # ===========================================================================================\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "# from keras.models import Sequential\n",
    "# from keras.losses import Huber\n",
    "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from prophet import Prophet\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import time\n",
    "# import pickle\n",
    "\n",
    "# def train_hybrid_model(resolution, target, verbose=True):\n",
    "#     \"\"\"\n",
    "#     UPGRADED HYBRID MODEL:\n",
    "#     - Data: Log Transformed + Storm Masking (Best Data Practice).\n",
    "#     - Model: Setup_1 Architecture (Scaler -1to1, Bottleneck LSTM).\n",
    "#     - Goal: Optimized for Autoscaling (Robust short-term forecasting).\n",
    "#     \"\"\"\n",
    "#     if verbose:\n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(f\"TRAINING: Hybrid Upgrade (Setup_1 Style) | {resolution} | {target}\")\n",
    "#         print(f\"{'='*70}\")\n",
    "    \n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # --- C·∫§U H√åNH ---\n",
    "#     # L·∫•y params (n·∫øu ch∆∞a c√≥ dict th√¨ d√πng default)\n",
    "#     try:\n",
    "#         params = RESOLUTION_PARAMS[resolution]\n",
    "#     except:\n",
    "#         params = {'window': 12, 'lstm_units': 50, 'epochs': 50, 'batch_size': 16}\n",
    "        \n",
    "#     window = params['window']\n",
    "#     lstm_units = params['lstm_units'] # Layer 1\n",
    "#     lstm_units_2 = 20                 # Layer 2 (Bottleneck t·ª´ Setup_1)\n",
    "#     epochs = params['epochs']\n",
    "#     batch_size = params['batch_size']\n",
    "    \n",
    "#     results_dir = f\"{RESULTS_BASE_DIR}/{resolution}_{target}_upgrade\"\n",
    "#     os.makedirs(results_dir, exist_ok=True)\n",
    "#     os.makedirs(f\"{results_dir}/prophet_model\", exist_ok=True)\n",
    "    \n",
    "#     try:\n",
    "#         # ==================\n",
    "#         # 1. LOAD DATA & STORM MASKING (V·∫´n gi·ªØ ƒë·ªÉ tr√°nh h·ªçc r√°c t·ª´ b√£o)\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"[1/7] Loading data & Masking Storm...\")\n",
    "        \n",
    "#         train_df = pd.read_csv(f\"{DATA_DIR}/train_{resolution}.csv\", index_col=0, parse_dates=True)\n",
    "#         test_df = pd.read_csv(f\"{DATA_DIR}/test_{resolution}.csv\", index_col=0, parse_dates=True)\n",
    "        \n",
    "#         y_test_raw_true = test_df[target].values\n",
    "\n",
    "#         # X·ª≠ l√Ω b√£o (G√°n NaN)\n",
    "#         storm_start = pd.Timestamp(\"1995-08-01 14:52:01\")\n",
    "#         storm_end   = pd.Timestamp(\"1995-08-03 04:36:13\")\n",
    "#         mask = (train_df.index >= storm_start) & (train_df.index <= storm_end)\n",
    "#         train_df.loc[mask, target] = None \n",
    "        \n",
    "#         # Log Transform (Gi·ªØ l·∫°i v√¨ Autoscaling c·∫ßn x·ª≠ l√Ω spikes m∆∞·ª£t h∆°n)\n",
    "#         train_df[target] = np.log1p(train_df[target])\n",
    "#         test_df[target] = np.log1p(test_df[target])\n",
    "        \n",
    "#         full_df = pd.concat([train_df, test_df]).sort_index()\n",
    "#         train_size = len(train_df)\n",
    "        \n",
    "#         # ==================\n",
    "#         # 2. TRAIN PROPHET\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[2/7] Training Prophet...\")\n",
    "        \n",
    "#         prophet_train = prepare_prophet_data(train_df, target)\n",
    "        \n",
    "#         prophet_model = Prophet(\n",
    "#             changepoint_prior_scale=5.1, \n",
    "#             seasonality_prior_scale=30,\n",
    "#             seasonality_mode='multiplicative',\n",
    "#         )\n",
    "#         prophet_model.add_seasonality(name='daily_high_freq', period=1, fourier_order=50)\n",
    "#         prophet_model.add_seasonality(name='weekly_high_freq', period=7, fourier_order=20)\n",
    "#         prophet_model.add_regressor('hour')\n",
    "#         prophet_model.add_regressor('day_of_week')\n",
    "#         prophet_model.add_regressor('is_weekend')\n",
    "        \n",
    "#         prophet_model.fit(prophet_train)\n",
    "        \n",
    "#         prophet_full = prepare_prophet_data(full_df, target)\n",
    "#         prophet_forecast = prophet_model.predict(prophet_full[['ds', 'hour', 'day_of_week', 'is_weekend']])\n",
    "        \n",
    "#         # ==================\n",
    "#         # 3. COMPUTE RESIDUALS & PREPARE LSTM\n",
    "#         # ==================\n",
    "#         full_df['yhat_prophet'] = prophet_forecast['yhat'].values\n",
    "#         full_df['residual'] = full_df[target] - full_df['yhat_prophet']\n",
    "#         full_df['residual'] = full_df['residual'].fillna(0) # Fill 0 cho v√πng b√£o\n",
    "        \n",
    "#         # --- [UPGRADE T·ª™ SETUP_1]: Scaler (-1, 1) ---\n",
    "#         # T·ªët h∆°n cho residuals v√¨ d·ªØ li·ªáu ph√¢n b·ªë quanh 0\n",
    "#         scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        \n",
    "#         residual_train_values = full_df.iloc[:train_size][['residual']].values\n",
    "#         scaler.fit(residual_train_values)\n",
    "#         residual_train_scaled = scaler.transform(residual_train_values)\n",
    "        \n",
    "#         X_train, y_train = make_sequences(residual_train_scaled.flatten(), window)\n",
    "#         X_train = X_train.reshape(-1, window, 1)\n",
    "        \n",
    "#         # Split Valid\n",
    "#         val_ratio = 0.2\n",
    "#         val_size = int(len(X_train) * val_ratio)\n",
    "#         X_train_sub = X_train[:-val_size]\n",
    "#         y_train_sub = y_train[:-val_size]\n",
    "#         X_val = X_train[-val_size:]\n",
    "#         y_val = y_train[-val_size:]\n",
    "        \n",
    "#         # ==================\n",
    "#         # 5. TRAIN LSTM (UPGRADE: BOTTLENECK ARCHITECTURE)\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[5/7] Training LSTM (Setup_1 Architecture: 50 -> 20)...\")\n",
    "        \n",
    "#         # Ki·∫øn tr√∫c √©p m√¥ h√¨nh h·ªçc ƒë·∫∑c tr∆∞ng quan tr·ªçng (Setup_1)\n",
    "#         lstm_model = Sequential([\n",
    "#             Bidirectional(LSTM(lstm_units, return_sequences=True, input_shape=(window, 1))),\n",
    "#             Dropout(0.2),\n",
    "#             Bidirectional(LSTM(lstm_units_2, return_sequences=False)), # Gi·∫£m xu·ªëng 20 units\n",
    "#             Dense(1)\n",
    "#         ])\n",
    "        \n",
    "#         # Huber Loss v·∫´n l√† ch√¢n √°i cho Autoscaling (ch·ªëng nhi·ªÖu t·ªët h∆°n MSE)\n",
    "#         lstm_model.compile(optimizer='adam', loss=Huber(delta=1.0), metrics=['mae'])\n",
    "        \n",
    "#         callbacks = [\n",
    "#             EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss', verbose=1),\n",
    "#             ReduceLROnPlateau(patience=5, factor=0.5, monitor='val_loss', verbose=1)\n",
    "#         ]\n",
    "        \n",
    "#         history = lstm_model.fit(\n",
    "#             X_train_sub, y_train_sub,\n",
    "#             validation_data=(X_val, y_val),\n",
    "#             epochs=epochs,\n",
    "#             batch_size=batch_size,\n",
    "#             callbacks=callbacks,\n",
    "#             verbose=1\n",
    "#         )\n",
    "        \n",
    "#         # ==================\n",
    "#         # 6. HYBRID PREDICTION\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[6/7] Generating predictions...\")\n",
    "        \n",
    "#         # Predict Residuals (All data)\n",
    "#         residual_all_values = full_df[['residual']].values\n",
    "#         residual_all_scaled = scaler.transform(residual_all_values)\n",
    "        \n",
    "#         X_all, _ = make_sequences(residual_all_scaled.flatten(), window)\n",
    "#         X_all = X_all.reshape(-1, window, 1)\n",
    "        \n",
    "#         residual_pred_scaled = lstm_model.predict(X_all, verbose=0)\n",
    "#         residual_pred_log = scaler.inverse_transform(residual_pred_scaled).flatten()\n",
    "        \n",
    "#         # Reconstruct Hybrid (Log -> Exp)\n",
    "#         prophet_pred_log = full_df['yhat_prophet'].iloc[window:].values\n",
    "#         hybrid_pred_log = prophet_pred_log + residual_pred_log\n",
    "        \n",
    "#         hybrid_pred_final = np.expm1(hybrid_pred_log)\n",
    "#         prophet_pred_final = np.expm1(prophet_pred_log)\n",
    "        \n",
    "#         # ==================\n",
    "#         # 7. EVALUATION\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[7/7] Evaluating...\")\n",
    "\n",
    "#         lstm_test_start_idx = train_size - window\n",
    "        \n",
    "#         hybrid_test_pred = hybrid_pred_final[lstm_test_start_idx:]\n",
    "#         prophet_test_pred = prophet_pred_final[lstm_test_start_idx:]\n",
    "#         residual_test_pred = residual_pred_log[lstm_test_start_idx:] \n",
    "\n",
    "#         residual_test_true = full_df['residual'].iloc[train_size:].values\n",
    "        \n",
    "#         min_len = min(len(y_test_raw_true), len(hybrid_test_pred), len(residual_test_true))\n",
    "        \n",
    "#         y_test_true = y_test_raw_true[:min_len]\n",
    "#         hybrid_test_pred = hybrid_test_pred[:min_len]\n",
    "#         prophet_test_pred = prophet_test_pred[:min_len]\n",
    "#         residual_test_true = residual_test_true[:min_len]\n",
    "#         residual_test_pred = residual_test_pred[:min_len]\n",
    "        \n",
    "#         # Metrics\n",
    "#         prophet_metrics = calculate_metrics(y_test_true, prophet_test_pred, \"Prophet\")\n",
    "#         hybrid_metrics = calculate_metrics(y_test_true, hybrid_test_pred, \"Hybrid\")\n",
    "        \n",
    "#         # Residual MAPE\n",
    "#         residual_mape = np.mean(\n",
    "#             np.abs((residual_test_true - residual_test_pred) / (np.abs(residual_test_true) + 1e-8))\n",
    "#         ) * 100\n",
    "        \n",
    "#         if verbose:\n",
    "#             print(f\"\\n>>> RESULT (Resolution: {resolution}) - UPGRADED <<<\")\n",
    "#             print(\"-\" * 60)\n",
    "#             print(f\" LSTM Residuals - MAPE: {residual_mape:.2f}%\")\n",
    "#             print(\"-\" * 60)\n",
    "#             print(f\" Prophet - MAE: {prophet_metrics['MAE']:.2f} | MAPE: {prophet_metrics['MAPE']:.2f}%\")\n",
    "#             print(f\" Hybrid  - MAE: {hybrid_metrics['MAE']:.2f} | MAPE: {hybrid_metrics['MAPE']:.2f}%\")\n",
    "        \n",
    "#         # Save results\n",
    "#         elapsed_time = time.time() - start_time\n",
    "#         return {\n",
    "#             'resolution': resolution,\n",
    "#             'target': target,\n",
    "#             'hybrid_mae': hybrid_metrics['MAE'],\n",
    "#             'hybrid_mape': hybrid_metrics['MAPE'],\n",
    "#             'residual_mape': residual_mape,\n",
    "#             'training_time_sec': elapsed_time\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:14:03.541426Z",
     "iopub.status.busy": "2026-02-02T13:14:03.541137Z",
     "iopub.status.idle": "2026-02-02T13:14:03.565628Z",
     "shell.execute_reply": "2026-02-02T13:14:03.564927Z",
     "shell.execute_reply.started": "2026-02-02T13:14:03.541406Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # ===========================================================================================\n",
    "# # CELL 5: SETUP_1 REVIVAL (RAW DATA + STORM MASKING + ONE-STEP AHEAD)\n",
    "# # ===========================================================================================\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "# from keras.models import Sequential\n",
    "# from keras.losses import Huber\n",
    "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from prophet import Prophet\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import time\n",
    "# import pickle\n",
    "\n",
    "# def train_hybrid_model(resolution, target, verbose=True):\n",
    "#     if verbose:\n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(f\"TRAINING: Setup_1 Revival (RAW DATA + One-Step Ahead) | {resolution} | {target}\")\n",
    "#         print(f\"{'='*70}\")\n",
    "    \n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # --- C·∫§U H√åNH ---\n",
    "#     try: params = RESOLUTION_PARAMS[resolution]\n",
    "#     except: params = {'window': 12, 'lstm_units': 50, 'epochs': 50, 'batch_size': 16}\n",
    "        \n",
    "#     window = params['window']\n",
    "#     lstm_units = params['lstm_units'] \n",
    "#     lstm_units_2 = 20  # Bottleneck t·ª´ Setup_1\n",
    "#     epochs = params['epochs']\n",
    "#     batch_size = params['batch_size']\n",
    "    \n",
    "#     results_dir = f\"{RESULTS_BASE_DIR}/{resolution}_{target}_setup1_revival\"\n",
    "#     os.makedirs(results_dir, exist_ok=True)\n",
    "#     os.makedirs(f\"{results_dir}/prophet_model\", exist_ok=True)\n",
    "    \n",
    "#     try:\n",
    "#         # ==================\n",
    "#         # 1. LOAD DATA & STORM MASKING\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"[1/7] Loading Data (RAW)...\")\n",
    "#         train_df = pd.read_csv(f\"{DATA_DIR}/train_{resolution}.csv\", index_col=0, parse_dates=True)\n",
    "#         test_df = pd.read_csv(f\"{DATA_DIR}/test_{resolution}.csv\", index_col=0, parse_dates=True)\n",
    "        \n",
    "#         y_test_raw_true = test_df[target].values\n",
    "\n",
    "#         # Masking Storm\n",
    "#         storm_start = pd.Timestamp(\"1995-08-01 14:52:01\")\n",
    "#         storm_end   = pd.Timestamp(\"1995-08-03 04:36:13\")\n",
    "#         mask = (train_df.index >= storm_start) & (train_df.index <= storm_end)\n",
    "#         train_df.loc[mask, target] = None \n",
    "        \n",
    "#         # --- QUAN TR·ªåNG: KH√îNG D√ôNG LOG TRANSFORM ---\n",
    "#         # Ch·∫°y tr·ª±c ti·∫øp tr√™n Raw Data\n",
    "        \n",
    "#         full_df = pd.concat([train_df, test_df]).sort_index()\n",
    "#         train_size = len(train_df)\n",
    "        \n",
    "#         # ==================\n",
    "#         # 2. TRAIN PROPHET\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[2/7] Training Prophet (Raw Space)...\")\n",
    "#         prophet_train = prepare_prophet_data(train_df, target)\n",
    "        \n",
    "#         prophet_model = Prophet(\n",
    "#             changepoint_prior_scale=5.1, \n",
    "#             seasonality_prior_scale=30,\n",
    "#             seasonality_mode='multiplicative', # Raw data bi·∫øn ƒë·ªông m·∫°nh n√™n th·ª≠ multiplicative\n",
    "#         )\n",
    "#         prophet_model.add_seasonality(name='daily_high_freq', period=1, fourier_order=50)\n",
    "#         prophet_model.add_seasonality(name='weekly_high_freq', period=7, fourier_order=20)\n",
    "#         prophet_model.add_regressor('hour')\n",
    "#         prophet_model.add_regressor('day_of_week')\n",
    "#         prophet_model.add_regressor('is_weekend')\n",
    "        \n",
    "#         prophet_model.fit(prophet_train)\n",
    "        \n",
    "#         prophet_full = prepare_prophet_data(full_df, target)\n",
    "#         prophet_forecast = prophet_model.predict(prophet_full[['ds', 'hour', 'day_of_week', 'is_weekend']])\n",
    "        \n",
    "#         # ==================\n",
    "#         # 3. COMPUTE RESIDUALS\n",
    "#         # ==================\n",
    "#         full_df['yhat_prophet'] = prophet_forecast['yhat'].values\n",
    "#         # Residual = Th·ª±c t·∫ø - Prophet\n",
    "#         full_df['residual'] = full_df[target] - full_df['yhat_prophet']\n",
    "#         full_df['residual'] = full_df['residual'].fillna(0) \n",
    "        \n",
    "#         # --- SCALER (-1, 1) C·ª¶A SETUP_1 ---\n",
    "#         scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        \n",
    "#         residual_train_values = full_df.iloc[:train_size][['residual']].values\n",
    "#         scaler.fit(residual_train_values)\n",
    "#         residual_train_scaled = scaler.transform(residual_train_values)\n",
    "        \n",
    "#         X_train, y_train = make_sequences(residual_train_scaled.flatten(), window)\n",
    "#         X_train = X_train.reshape(-1, window, 1)\n",
    "        \n",
    "#         # Split Valid\n",
    "#         val_size = int(len(X_train) * 0.2)\n",
    "#         X_train_sub, y_train_sub = X_train[:-val_size], y_train[:-val_size]\n",
    "#         X_val, y_val = X_train[-val_size:], y_train[-val_size:]\n",
    "        \n",
    "#         # ==================\n",
    "#         # 5. TRAIN LSTM (SETUP_1 ARCHITECTURE)\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[5/7] Training LSTM (Bottleneck 50->20)...\")\n",
    "        \n",
    "#         lstm_model = Sequential([\n",
    "#             LSTM(lstm_units, return_sequences=True, input_shape=(window, 1)),\n",
    "#             Dropout(0.2),\n",
    "#             LSTM(lstm_units, return_sequences=False), # Bottleneck\n",
    "#             Dense(1)\n",
    "#         ])\n",
    "        \n",
    "#         lstm_model.compile(optimizer='adam', loss='huber', metrics=['mae'])\n",
    "        \n",
    "#         lstm_model.fit(X_train_sub, y_train_sub, validation_data=(X_val, y_val), \n",
    "#                        epochs=epochs, batch_size=batch_size, verbose=1,\n",
    "#                        callbacks=[EarlyStopping(patience=10, restore_best_weights=True)])\n",
    "        \n",
    "#         # ==================\n",
    "#         # 6. HYBRID PREDICTION (ONE-STEP LOGIC)\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[6/7] Generating Forecast (Raw Space)...\")\n",
    "        \n",
    "#         # T·∫°o chu·ªói input cho Test: ƒêu√¥i Train + Residual Test Th·ª±c T·∫ø\n",
    "#         residual_test_true = full_df['residual'].iloc[train_size:].values\n",
    "#         residual_train_tail = full_df['residual'].iloc[train_size-window:train_size].values\n",
    "        \n",
    "#         # N·ªëi l·∫°i\n",
    "#         history_stream = np.concatenate([residual_train_tail, residual_test_true])\n",
    "#         history_stream_scaled = scaler.transform(history_stream.reshape(-1, 1)).flatten()\n",
    "        \n",
    "#         # T·∫°o X_test d·∫°ng cu·ªôn (Vectorized rolling window - Nhanh h∆°n v√≤ng l·∫∑p for)\n",
    "#         # H√†m make_sequences s·∫Ω t·ª± ƒë·ªông t·∫°o c√°c c·ª≠a s·ªï tr∆∞·ª£t [t-window : t]\n",
    "#         X_test_rolling, _ = make_sequences(history_stream_scaled, window)\n",
    "#         X_test_rolling = X_test_rolling.reshape(-1, window, 1)\n",
    "        \n",
    "#         # Predict Residuals (Raw Scaled)\n",
    "#         residual_test_pred_scaled = lstm_model.predict(X_test_rolling, verbose=0)\n",
    "        \n",
    "#         # Inverse Scale -> Raw Residuals\n",
    "#         residual_test_pred_raw = scaler.inverse_transform(residual_test_pred_scaled).flatten()\n",
    "        \n",
    "#         # C·ªông l·∫°i: Hybrid = Prophet (Raw) + LSTM Residual (Raw)\n",
    "#         prophet_test_pred_raw = full_df['yhat_prophet'].iloc[train_size:].values\n",
    "        \n",
    "#         # C·∫Øt kh·ªõp ƒë·ªô d√†i\n",
    "#         min_len = min(len(prophet_test_pred_raw), len(residual_test_pred_raw))\n",
    "#         hybrid_test_pred_final = prophet_test_pred_raw[:min_len] + residual_test_pred_raw[:min_len]\n",
    "        \n",
    "#         # K·∫πp gi√° tr·ªã √¢m v·ªÅ 0 (n·∫øu c√≥)\n",
    "#         hybrid_test_pred_final = np.maximum(hybrid_test_pred_final, 0)\n",
    "\n",
    "#         # ==================\n",
    "#         # 7. EVALUATION\n",
    "#         # ==================\n",
    "#         if verbose: print(f\"\\n[7/7] Evaluating...\")\n",
    "\n",
    "#         # C·∫Øt d·ªØ li·ªáu th·ª±c t·∫ø v√† d·ª± b√°o cho kh·ªõp nhau\n",
    "#         y_test_true = y_test_raw_true[:min_len]\n",
    "#         prophet_test_pred = prophet_test_pred_raw[:min_len]\n",
    "#         hybrid_test_pred = hybrid_test_pred_final[:min_len]\n",
    "        \n",
    "#         # T√≠nh metrics\n",
    "#         prophet_metrics = calculate_metrics(y_test_true, prophet_test_pred, \"Prophet\")\n",
    "#         hybrid_metrics = calculate_metrics(y_test_true, hybrid_test_pred, \"Hybrid\")\n",
    "        \n",
    "#         if verbose:\n",
    "#             print(f\"\\n>>> RESULT (Resolution: {resolution}) - NO LOG TRANSFORM <<<\")\n",
    "#             print(\"-\" * 60)\n",
    "#             print(\n",
    "#                 f\" Prophet - MAE: {prophet_metrics['MAE']:.2f}, \"\n",
    "#                 f\"MSE: {prophet_metrics['MSE']:.2f}, \"\n",
    "#                 f\"MAPE: {prophet_metrics['MAPE']:.2f}%, \"\n",
    "#                 f\"RMSE: {prophet_metrics['RMSE']:.2f}\"\n",
    "#             )\n",
    "#             print(\n",
    "#                 f\" Hybrid  - MAE: {hybrid_metrics['MAE']:.2f}, \"\n",
    "#                 f\"MSE: {hybrid_metrics['MSE']:.2f}, \"\n",
    "#                 f\"MAPE: {hybrid_metrics['MAPE']:.2f}%, \"\n",
    "#                 f\"RMSE: {hybrid_metrics['RMSE']:.2f}\"\n",
    "#             )\n",
    "        \n",
    "#         # Save results\n",
    "#         # L∆∞u CSV ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì\n",
    "#         test_timestamps = test_df.index[:min_len]\n",
    "#         predictions_df = pd.DataFrame({\n",
    "#             'timestamp': test_timestamps,\n",
    "#             'actual': y_test_true,\n",
    "#             'prophet_pred': prophet_test_pred,\n",
    "#             'hybrid_pred': hybrid_test_pred\n",
    "#         })\n",
    "#         predictions_df.to_csv(f\"{results_dir}/hybrid_predictions.csv\", index=False)\n",
    "\n",
    "#         elapsed_time = time.time() - start_time\n",
    "#         return {\n",
    "#             'resolution': resolution,\n",
    "#             'target': target,\n",
    "#             'prophet_mae': prophet_metrics['MAE'],\n",
    "#             'prophet_rmse': prophet_metrics['RMSE'],\n",
    "#             'prophet_mse': prophet_metrics['MSE'],\n",
    "#             'prophet_mape': prophet_metrics['MAPE'],\n",
    "#             'hybrid_mae': hybrid_metrics['MAE'],\n",
    "#             'hybrid_rmse': hybrid_metrics['RMSE'],\n",
    "#             'hybrid_mse': hybrid_metrics['MSE'],\n",
    "#             'hybrid_mape': hybrid_metrics['MAPE'],\n",
    "#             'hybrid_r2': hybrid_metrics['R2'],\n",
    "#             'results_dir': results_dir\n",
    "#         }\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:22:50.223137Z",
     "iopub.status.busy": "2026-02-02T13:22:50.222846Z",
     "iopub.status.idle": "2026-02-02T13:22:50.244936Z",
     "shell.execute_reply": "2026-02-02T13:22:50.243974Z",
     "shell.execute_reply.started": "2026-02-02T13:22:50.223113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===========================================================================================\n",
    "# CELL 5: SETUP_1 REPLICA (RAW DATA + STORM MASKING + EXACT LSTM ARCHITECTURE)\n",
    "# ===========================================================================================\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import Bidirectional, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.losses import Huber\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "def train_hybrid_model(resolution, target, verbose=True):\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"TRAINING: Setup_1 Replica (Exact Match) | {resolution} | {target}\")\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # --- C·∫§U H√åNH ---\n",
    "    try: params = RESOLUTION_PARAMS[resolution]\n",
    "    except: params = {'window': 12, 'lstm_units': 50, 'epochs': 50, 'batch_size': 16}\n",
    "        \n",
    "    window = params['window'] # Ch√≠nh l√† look_back trong code c·ªßa b·∫°n\n",
    "    epochs = params['epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    \n",
    "    results_dir = f\"{RESULTS_BASE_DIR}/{resolution}_{target}_setup1_replica\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    os.makedirs(f\"{results_dir}/prophet_model\", exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # ==================\n",
    "        # 1. LOAD DATA & STORM MASKING\n",
    "        # ==================\n",
    "        if verbose: print(f\"[1/7] Loading Data (RAW)...\")\n",
    "        train_df = pd.read_csv(f\"{DATA_DIR}/train_{resolution}.csv\", index_col=0, parse_dates=True)\n",
    "        test_df = pd.read_csv(f\"{DATA_DIR}/test_{resolution}.csv\", index_col=0, parse_dates=True)\n",
    "        \n",
    "        y_test_raw_true = test_df[target].values\n",
    "\n",
    "        # Masking Storm\n",
    "        storm_start = pd.Timestamp(\"1995-08-01 14:52:01\")\n",
    "        storm_end   = pd.Timestamp(\"1995-08-03 04:36:13\")\n",
    "        mask = (train_df.index >= storm_start) & (train_df.index <= storm_end)\n",
    "        train_df.loc[mask, target] = None \n",
    "        \n",
    "        # KH√îNG D√ôNG LOG (Raw Data)\n",
    "        \n",
    "        full_df = pd.concat([train_df, test_df]).sort_index()\n",
    "        train_size = len(train_df)\n",
    "        \n",
    "        # ==================\n",
    "        # 2. TRAIN PROPHET\n",
    "        # ==================\n",
    "        if verbose: print(f\"\\n[2/7] Training Prophet (Raw Space)...\")\n",
    "        prophet_train = prepare_prophet_data(train_df, target)\n",
    "        \n",
    "        prophet_model = Prophet(\n",
    "            changepoint_prior_scale=5.1, \n",
    "            seasonality_prior_scale=30,\n",
    "            seasonality_mode='multiplicative',\n",
    "        )\n",
    "        prophet_model.add_seasonality(name='daily_high_freq', period=1, fourier_order=50)\n",
    "        prophet_model.add_seasonality(name='weekly_high_freq', period=7, fourier_order=20)\n",
    "        prophet_model.add_regressor('hour')\n",
    "        prophet_model.add_regressor('day_of_week')\n",
    "        prophet_model.add_regressor('is_weekend')\n",
    "        \n",
    "        prophet_model.fit(prophet_train)\n",
    "        \n",
    "        prophet_full = prepare_prophet_data(full_df, target)\n",
    "        prophet_forecast = prophet_model.predict(prophet_full[['ds', 'hour', 'day_of_week', 'is_weekend']])\n",
    "        \n",
    "        # ==================\n",
    "        # 3. COMPUTE RESIDUALS\n",
    "        # ==================\n",
    "        full_df['yhat_prophet'] = prophet_forecast['yhat'].values\n",
    "        full_df['residual'] = full_df[target] - full_df['yhat_prophet']\n",
    "        full_df['residual'] = full_df['residual'].fillna(0) \n",
    "        \n",
    "        # --- SCALER (-1, 1) C·ª¶A SETUP_1 ---\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        \n",
    "        residual_train_values = full_df.iloc[:train_size][['residual']].values\n",
    "        scaler.fit(residual_train_values)\n",
    "        residual_train_scaled = scaler.transform(residual_train_values)\n",
    "        \n",
    "        X_train, y_train = make_sequences(residual_train_scaled.flatten(), window)\n",
    "        X_train = X_train.reshape(-1, window, 1)\n",
    "        \n",
    "        # Split Valid\n",
    "        val_size = int(len(X_train) * 0.2)\n",
    "        X_train_sub, y_train_sub = X_train[:-val_size], y_train[:-val_size]\n",
    "        X_val, y_val = X_train[-val_size:], y_train[-val_size:]\n",
    "        \n",
    "        # ==================\n",
    "        # 5. TRAIN LSTM (EXACT SETUP_1 SYNTAX)\n",
    "        # ==================\n",
    "        if verbose: print(f\"\\n[5/7] Training LSTM (Exact Setup_1 Code)...\")\n",
    "        \n",
    "        # --- [CH·ªàNH S·ª¨A] GI·ªêNG H·ªÜT SETUP_1 ---\n",
    "        lstm_model = Sequential()\n",
    "        # Layer 1: 50 units\n",
    "        lstm_model.add(Bidirectional(LSTM(50, return_sequences=True), input_shape=(window, 1)))\n",
    "        lstm_model.add(Dropout(0.2))\n",
    "        # Layer 2: 20 units (Bottleneck)\n",
    "        lstm_model.add(Bidirectional(LSTM(20))) # M·∫∑c ƒë·ªãnh return_sequences=False\n",
    "        lstm_model.add(Dense(1))\n",
    "        \n",
    "        # Compile: d√πng string 'huber' v√† 'adam' cho gi·ªëng Setup_1\n",
    "        lstm_model.compile(loss='huber', optimizer='adam') \n",
    "        \n",
    "        # Fit\n",
    "        lstm_model.fit(X_train_sub, y_train_sub, validation_data=(X_val, y_val), \n",
    "                       epochs=epochs, batch_size=batch_size, verbose=1,\n",
    "                       callbacks=[EarlyStopping(patience=10, restore_best_weights=True)])\n",
    "        \n",
    "        # ==================\n",
    "        # 6. HYBRID PREDICTION (VARIABLE NAMES MATCH SETUP_1)\n",
    "        # ==================\n",
    "        if verbose: print(f\"\\n[6/7] Generating Forecast (Concatenate Style)...\")\n",
    "        \n",
    "        # 1. L·∫•y d·ªØ li·ªáu\n",
    "        residual_test_true = full_df['residual'].iloc[train_size:].values\n",
    "        # L·∫•y ƒëu√¥i train (ch√≠nh l√† train_residuals[-look_back:] trong Setup_1)\n",
    "        residual_train_tail = full_df['residual'].iloc[train_size-window:train_size].values\n",
    "        \n",
    "        # 2. N·ªëi l·∫°i (GI·ªêNG BI·∫æN total_residuals)\n",
    "        # total_residuals = np.concatenate((train_residuals[-look_back:], test_residuals_true))\n",
    "        total_residuals = np.concatenate([residual_train_tail, residual_test_true])\n",
    "        \n",
    "        # 3. Scale (-1, 1)\n",
    "        # total_residuals_scaled = resid_scaler.transform(...)\n",
    "        total_residuals_scaled = scaler.transform(total_residuals.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # 4. T·∫°o X_test (Sliding Window)\n",
    "        X_test_rolling, _ = make_sequences(total_residuals_scaled, window)\n",
    "        X_test_rolling = X_test_rolling.reshape(-1, window, 1)\n",
    "        \n",
    "        # 5. Predict\n",
    "        residual_test_pred_scaled = lstm_model.predict(X_test_rolling, verbose=0)\n",
    "        \n",
    "        # 6. Inverse Scale\n",
    "        residual_test_pred_raw = scaler.inverse_transform(residual_test_pred_scaled).flatten()\n",
    "        \n",
    "        # 7. C·ªông l·∫°i: Prophet + LSTM\n",
    "        prophet_test_pred_raw = full_df['yhat_prophet'].iloc[train_size:].values\n",
    "        \n",
    "        # C·∫Øt kh·ªõp\n",
    "        min_len = min(len(prophet_test_pred_raw), len(residual_test_pred_raw))\n",
    "        hybrid_test_pred_final = prophet_test_pred_raw[:min_len] + residual_test_pred_raw[:min_len]\n",
    "        \n",
    "        # K·∫πp gi√° tr·ªã √¢m v·ªÅ 0 (Setup_1 c√≥ b∆∞·ªõc n√†y: np.maximum(final_pred, 0))\n",
    "        hybrid_test_pred_final = np.maximum(hybrid_test_pred_final, 0)\n",
    "\n",
    "        # ==================\n",
    "        # 7. EVALUATION\n",
    "        # ==================\n",
    "        if verbose: print(f\"\\n[7/7] Evaluating...\")\n",
    "\n",
    "        y_test_true = y_test_raw_true[:min_len]\n",
    "        prophet_test_pred = prophet_test_pred_raw[:min_len]\n",
    "        hybrid_test_pred = hybrid_test_pred_final[:min_len]\n",
    "        \n",
    "        # Metrics\n",
    "        prophet_metrics = calculate_metrics(y_test_true, prophet_test_pred, \"Prophet\")\n",
    "        hybrid_metrics = calculate_metrics(y_test_true, hybrid_test_pred, \"Hybrid\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n>>> RESULT (Resolution: {resolution}) - SETUP_1 REPLICA <<<\")\n",
    "            print(\"-\" * 60)\n",
    "            print(\n",
    "                f\" Prophet - MAE: {prophet_metrics['MAE']:.2f}, \"\n",
    "                f\"MSE: {prophet_metrics['MSE']:.2f}, \"\n",
    "                f\"MAPE: {prophet_metrics['MAPE']:.2f}%, \"\n",
    "                f\"RMSE: {prophet_metrics['RMSE']:.2f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\" Hybrid  - MAE: {hybrid_metrics['MAE']:.2f}, \"\n",
    "                f\"MSE: {hybrid_metrics['MSE']:.2f}, \"\n",
    "                f\"MAPE: {hybrid_metrics['MAPE']:.2f}%, \"\n",
    "                f\"RMSE: {hybrid_metrics['RMSE']:.2f}\"\n",
    "            )\n",
    "        \n",
    "        # Save results\n",
    "        test_timestamps = test_df.index[:min_len]\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'timestamp': test_timestamps,\n",
    "            'actual': y_test_true,\n",
    "            'prophet_pred': prophet_test_pred,\n",
    "            'hybrid_pred': hybrid_test_pred\n",
    "        })\n",
    "        predictions_df.to_csv(f\"{results_dir}/hybrid_predictions.csv\", index=False)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        return {\n",
    "            'resolution': resolution,\n",
    "            'target': target,\n",
    "            'prophet_mae': prophet_metrics['MAE'],\n",
    "            'prophet_rmse': prophet_metrics['RMSE'],\n",
    "            'prophet_mse': prophet_metrics['MSE'],\n",
    "            'prophet_mape': prophet_metrics['MAPE'],\n",
    "             'prophet_r2': prophet_metrics['R2'],\n",
    "            'hybrid_mae': hybrid_metrics['MAE'],\n",
    "            'hybrid_rmse': hybrid_metrics['RMSE'],\n",
    "            'hybrid_mse': hybrid_metrics['MSE'],\n",
    "            'hybrid_mape': hybrid_metrics['MAPE'],\n",
    "            'hybrid_r2': hybrid_metrics['R2'],\n",
    "            'results_dir': results_dir\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:22:51.403156Z",
     "iopub.status.busy": "2026-02-02T13:22:51.402869Z",
     "iopub.status.idle": "2026-02-02T13:27:28.158288Z",
     "shell.execute_reply": "2026-02-02T13:27:28.157716Z",
     "shell.execute_reply.started": "2026-02-02T13:22:51.403132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING AUTOMATED HYBRID TRAINING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "\n",
      "######################################################################\n",
      "CONFIGURATION 1/4\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "TRAINING: Setup_1 Replica (Exact Match) | 5min | request_count\n",
      "======================================================================\n",
      "[1/7] Loading Data (RAW)...\n",
      "\n",
      "[2/7] Training Prophet (Raw Space)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:22:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:23:27 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/7] Training LSTM (Exact Setup_1 Code)...\n",
      "Epoch 1/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - loss: 0.0049 - val_loss: 0.0020\n",
      "Epoch 2/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0027 - val_loss: 0.0020\n",
      "Epoch 3/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0026 - val_loss: 0.0020\n",
      "Epoch 4/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0020\n",
      "Epoch 5/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 6/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 7/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 8/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 9/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 10/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 11/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 12/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 13/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 14/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 15/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 16/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 17/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 18/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 19/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "Epoch 20/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0025 - val_loss: 0.0019\n",
      "\n",
      "[6/7] Generating Forecast (Concatenate Style)...\n",
      "\n",
      "[7/7] Evaluating...\n",
      "\n",
      ">>> RESULT (Resolution: 5min) - SETUP_1 REPLICA <<<\n",
      "------------------------------------------------------------\n",
      " Prophet - MAE: 57.08, MSE: 6615.84, MAPE: 36.60%, RMSE: 81.34\n",
      " Hybrid  - MAE: 34.96, MSE: 2208.31, MAPE: 27.71%, RMSE: 46.99\n",
      "\n",
      "‚úÖ Configuration 1/4 completed successfully\n",
      "\n",
      "üìä Progress: 1/4 (25.0%)\n",
      "   Elapsed: 1.5 min | Est. remaining: 4.5 min\n",
      "\n",
      "\n",
      "######################################################################\n",
      "CONFIGURATION 2/4\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "TRAINING: Setup_1 Replica (Exact Match) | 5min | total_bytes\n",
      "======================================================================\n",
      "[1/7] Loading Data (RAW)...\n",
      "\n",
      "[2/7] Training Prophet (Raw Space)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:24:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:24:47 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/7] Training LSTM (Exact Setup_1 Code)...\n",
      "Epoch 1/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - loss: 0.0056 - val_loss: 0.0039\n",
      "Epoch 2/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0048 - val_loss: 0.0038\n",
      "Epoch 3/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0047 - val_loss: 0.0037\n",
      "Epoch 4/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0047 - val_loss: 0.0037\n",
      "Epoch 5/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0047 - val_loss: 0.0037\n",
      "Epoch 6/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0047 - val_loss: 0.0037\n",
      "Epoch 7/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0047 - val_loss: 0.0037\n",
      "Epoch 8/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 9/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 10/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 11/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 12/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 13/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - loss: 0.0047 - val_loss: 0.0037\n",
      "Epoch 14/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 15/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 16/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 17/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 18/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "Epoch 19/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.0046 - val_loss: 0.0037\n",
      "\n",
      "[6/7] Generating Forecast (Concatenate Style)...\n",
      "\n",
      "[7/7] Evaluating...\n",
      "\n",
      ">>> RESULT (Resolution: 5min) - SETUP_1 REPLICA <<<\n",
      "------------------------------------------------------------\n",
      " Prophet - MAE: 928196.66, MSE: 1663852821668.35, MAPE: 38.37%, RMSE: 1289904.19\n",
      " Hybrid  - MAE: 632106.99, MSE: 717804046790.15, MAPE: 29.33%, RMSE: 847233.17\n",
      "\n",
      "‚úÖ Configuration 2/4 completed successfully\n",
      "\n",
      "üìä Progress: 2/4 (50.0%)\n",
      "   Elapsed: 2.8 min | Est. remaining: 2.8 min\n",
      "\n",
      "\n",
      "######################################################################\n",
      "CONFIGURATION 3/4\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "TRAINING: Setup_1 Replica (Exact Match) | 15min | request_count\n",
      "======================================================================\n",
      "[1/7] Loading Data (RAW)...\n",
      "\n",
      "[2/7] Training Prophet (Raw Space)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:25:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:25:46 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/7] Training LSTM (Exact Setup_1 Code)...\n",
      "Epoch 1/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - loss: 0.0249 - val_loss: 0.0017\n",
      "Epoch 2/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0028 - val_loss: 0.0017\n",
      "Epoch 3/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0028 - val_loss: 0.0017\n",
      "Epoch 4/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0028 - val_loss: 0.0017\n",
      "Epoch 5/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0027 - val_loss: 0.0017\n",
      "Epoch 6/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0026 - val_loss: 0.0017\n",
      "Epoch 7/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0026 - val_loss: 0.0017\n",
      "Epoch 8/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0026 - val_loss: 0.0017\n",
      "Epoch 9/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0026 - val_loss: 0.0017\n",
      "Epoch 10/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 11/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 12/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 13/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 14/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 15/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0017\n",
      "Epoch 16/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 17/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 18/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 19/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 20/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 21/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 22/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 23/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 24/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 25/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 26/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 27/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 28/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 29/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 30/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 31/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 32/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 33/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 34/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 35/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 36/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 37/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 38/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 39/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 40/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 41/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 42/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 43/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 44/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 45/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 46/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 47/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 48/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 49/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 50/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0023 - val_loss: 0.0016\n",
      "\n",
      "[6/7] Generating Forecast (Concatenate Style)...\n",
      "\n",
      "[7/7] Evaluating...\n",
      "\n",
      ">>> RESULT (Resolution: 15min) - SETUP_1 REPLICA <<<\n",
      "------------------------------------------------------------\n",
      " Prophet - MAE: 154.81, MSE: 51201.37, MAPE: 24.46%, RMSE: 226.28\n",
      " Hybrid  - MAE: 83.81, MSE: 13171.50, MAPE: 16.03%, RMSE: 114.77\n",
      "\n",
      "‚úÖ Configuration 3/4 completed successfully\n",
      "\n",
      "üìä Progress: 3/4 (75.0%)\n",
      "   Elapsed: 3.7 min | Est. remaining: 1.2 min\n",
      "\n",
      "\n",
      "######################################################################\n",
      "CONFIGURATION 4/4\n",
      "######################################################################\n",
      "\n",
      "======================================================================\n",
      "TRAINING: Setup_1 Replica (Exact Match) | 15min | total_bytes\n",
      "======================================================================\n",
      "[1/7] Loading Data (RAW)...\n",
      "\n",
      "[2/7] Training Prophet (Raw Space)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:26:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "13:26:38 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/7] Training LSTM (Exact Setup_1 Code)...\n",
      "Epoch 1/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - loss: 0.0108 - val_loss: 0.0045\n",
      "Epoch 2/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0052 - val_loss: 0.0045\n",
      "Epoch 3/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0050 - val_loss: 0.0044\n",
      "Epoch 4/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0050 - val_loss: 0.0044\n",
      "Epoch 5/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0050 - val_loss: 0.0043\n",
      "Epoch 6/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0050 - val_loss: 0.0043\n",
      "Epoch 7/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0049 - val_loss: 0.0043\n",
      "Epoch 8/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0049 - val_loss: 0.0043\n",
      "Epoch 9/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0049 - val_loss: 0.0044\n",
      "Epoch 10/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0049 - val_loss: 0.0043\n",
      "Epoch 11/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0049 - val_loss: 0.0043\n",
      "Epoch 12/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0049 - val_loss: 0.0043\n",
      "Epoch 13/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0049 - val_loss: 0.0043\n",
      "Epoch 14/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0044\n",
      "Epoch 15/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0044\n",
      "Epoch 16/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0049 - val_loss: 0.0043\n",
      "Epoch 17/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0049 - val_loss: 0.0044\n",
      "Epoch 18/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 19/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 20/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 21/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 22/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 23/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 24/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 25/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 26/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 27/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 28/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 29/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0047 - val_loss: 0.0043\n",
      "Epoch 30/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 31/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 32/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 33/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 34/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0042\n",
      "Epoch 35/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 36/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 37/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0042\n",
      "Epoch 38/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0042\n",
      "Epoch 39/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 40/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0042\n",
      "Epoch 41/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0042\n",
      "Epoch 42/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 43/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0042\n",
      "Epoch 44/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 45/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 46/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0048 - val_loss: 0.0042\n",
      "Epoch 47/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 48/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 49/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 50/50\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0047 - val_loss: 0.0042\n",
      "\n",
      "[6/7] Generating Forecast (Concatenate Style)...\n",
      "\n",
      "[7/7] Evaluating...\n",
      "\n",
      ">>> RESULT (Resolution: 15min) - SETUP_1 REPLICA <<<\n",
      "------------------------------------------------------------\n",
      " Prophet - MAE: 2491716.33, MSE: 12148277865859.69, MAPE: 30.77%, RMSE: 3485437.97\n",
      " Hybrid  - MAE: 1502758.73, MSE: 3992072664394.16, MAPE: 22.56%, RMSE: 1998017.18\n",
      "\n",
      "‚úÖ Configuration 4/4 completed successfully\n",
      "\n",
      "üìä Progress: 4/4 (100.0%)\n",
      "   Elapsed: 4.6 min | Est. remaining: 0.0 min\n",
      "\n",
      "======================================================================\n",
      "ALL CONFIGURATIONS COMPLETED\n",
      "======================================================================\n",
      "  Total time: 4.6 minutes\n",
      "  Successful: 4/4\n",
      "  Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# CELL 6: RUN ALL CONFIGURATIONS\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING AUTOMATED HYBRID TRAINING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_results = []\n",
    "total_configs = len(RESOLUTIONS) * len(TARGETS)\n",
    "current_config = 0\n",
    "\n",
    "pipeline_start_time = time.time()\n",
    "\n",
    "for resolution in RESOLUTIONS:\n",
    "    for target in TARGETS:\n",
    "        current_config += 1\n",
    "        \n",
    "        print(f\"\\n\\n{'#'*70}\")\n",
    "        print(f\"CONFIGURATION {current_config}/{total_configs}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "        \n",
    "        result = train_hybrid_model(resolution, target, verbose=True)\n",
    "        \n",
    "        if result is not None:\n",
    "            all_results.append(result)\n",
    "            print(f\"\\n‚úÖ Configuration {current_config}/{total_configs} completed successfully\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Configuration {current_config}/{total_configs} failed\")\n",
    "        \n",
    "        # Progress update\n",
    "        elapsed = time.time() - pipeline_start_time\n",
    "        avg_time = elapsed / current_config\n",
    "        remaining = (total_configs - current_config) * avg_time\n",
    "        \n",
    "        print(f\"\\nüìä Progress: {current_config}/{total_configs} ({current_config/total_configs*100:.1f}%)\")\n",
    "        print(f\"   Elapsed: {elapsed/60:.1f} min | Est. remaining: {remaining/60:.1f} min\")\n",
    "\n",
    "total_elapsed = time.time() - pipeline_start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL CONFIGURATIONS COMPLETED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Total time: {total_elapsed/60:.1f} minutes\")\n",
    "print(f\"  Successful: {len(all_results)}/{total_configs}\")\n",
    "print(f\"  Failed: {total_configs - len(all_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:28:18.562584Z",
     "iopub.status.busy": "2026-02-02T13:28:18.562252Z",
     "iopub.status.idle": "2026-02-02T13:28:18.586509Z",
     "shell.execute_reply": "2026-02-02T13:28:18.585727Z",
     "shell.execute_reply.started": "2026-02-02T13:28:18.562559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GENERATING COMPREHENSIVE BENCHMARK\n",
      "======================================================================\n",
      "\n",
      "‚úì Benchmark saved: /kaggle/working//FINAL_BENCHMARK/comprehensive_comparison.csv\n",
      "\n",
      "üìä BENCHMARK RESULTS:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4795f_row0_col2, #T_4795f_row0_col7, #T_4795f_row0_col8, #T_4795f_row2_col2, #T_4795f_row2_col7, #T_4795f_row2_col8 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_4795f_row1_col2 {\n",
       "  background-color: #cbe982;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4795f_row1_col7 {\n",
       "  background-color: #e0f295;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4795f_row1_col8 {\n",
       "  background-color: #e2f397;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4795f_row3_col2, #T_4795f_row3_col7, #T_4795f_row3_col8 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4795f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4795f_level0_col0\" class=\"col_heading level0 col0\" >resolution</th>\n",
       "      <th id=\"T_4795f_level0_col1\" class=\"col_heading level0 col1\" >target</th>\n",
       "      <th id=\"T_4795f_level0_col2\" class=\"col_heading level0 col2\" >prophet_mae</th>\n",
       "      <th id=\"T_4795f_level0_col3\" class=\"col_heading level0 col3\" >prophet_rmse</th>\n",
       "      <th id=\"T_4795f_level0_col4\" class=\"col_heading level0 col4\" >prophet_mse</th>\n",
       "      <th id=\"T_4795f_level0_col5\" class=\"col_heading level0 col5\" >prophet_mape</th>\n",
       "      <th id=\"T_4795f_level0_col6\" class=\"col_heading level0 col6\" >prophet_r2</th>\n",
       "      <th id=\"T_4795f_level0_col7\" class=\"col_heading level0 col7\" >hybrid_mae</th>\n",
       "      <th id=\"T_4795f_level0_col8\" class=\"col_heading level0 col8\" >hybrid_rmse</th>\n",
       "      <th id=\"T_4795f_level0_col9\" class=\"col_heading level0 col9\" >hybrid_mse</th>\n",
       "      <th id=\"T_4795f_level0_col10\" class=\"col_heading level0 col10\" >hybrid_mape</th>\n",
       "      <th id=\"T_4795f_level0_col11\" class=\"col_heading level0 col11\" >hybrid_r2</th>\n",
       "      <th id=\"T_4795f_level0_col12\" class=\"col_heading level0 col12\" >results_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4795f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_4795f_row0_col0\" class=\"data row0 col0\" >5min</td>\n",
       "      <td id=\"T_4795f_row0_col1\" class=\"data row0 col1\" >request_count</td>\n",
       "      <td id=\"T_4795f_row0_col2\" class=\"data row0 col2\" >57.08</td>\n",
       "      <td id=\"T_4795f_row0_col3\" class=\"data row0 col3\" >81.34</td>\n",
       "      <td id=\"T_4795f_row0_col4\" class=\"data row0 col4\" >6615.84</td>\n",
       "      <td id=\"T_4795f_row0_col5\" class=\"data row0 col5\" >36.60%</td>\n",
       "      <td id=\"T_4795f_row0_col6\" class=\"data row0 col6\" >0.5584</td>\n",
       "      <td id=\"T_4795f_row0_col7\" class=\"data row0 col7\" >34.96</td>\n",
       "      <td id=\"T_4795f_row0_col8\" class=\"data row0 col8\" >46.99</td>\n",
       "      <td id=\"T_4795f_row0_col9\" class=\"data row0 col9\" >2208.31</td>\n",
       "      <td id=\"T_4795f_row0_col10\" class=\"data row0 col10\" >27.71%</td>\n",
       "      <td id=\"T_4795f_row0_col11\" class=\"data row0 col11\" >0.8526</td>\n",
       "      <td id=\"T_4795f_row0_col12\" class=\"data row0 col12\" >/kaggle/working//5min_request_count_setup1_replica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4795f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_4795f_row1_col0\" class=\"data row1 col0\" >5min</td>\n",
       "      <td id=\"T_4795f_row1_col1\" class=\"data row1 col1\" >total_bytes</td>\n",
       "      <td id=\"T_4795f_row1_col2\" class=\"data row1 col2\" >928196.66</td>\n",
       "      <td id=\"T_4795f_row1_col3\" class=\"data row1 col3\" >1289904.19</td>\n",
       "      <td id=\"T_4795f_row1_col4\" class=\"data row1 col4\" >1663852821668.35</td>\n",
       "      <td id=\"T_4795f_row1_col5\" class=\"data row1 col5\" >38.37%</td>\n",
       "      <td id=\"T_4795f_row1_col6\" class=\"data row1 col6\" >0.3530</td>\n",
       "      <td id=\"T_4795f_row1_col7\" class=\"data row1 col7\" >632106.99</td>\n",
       "      <td id=\"T_4795f_row1_col8\" class=\"data row1 col8\" >847233.17</td>\n",
       "      <td id=\"T_4795f_row1_col9\" class=\"data row1 col9\" >717804046790.15</td>\n",
       "      <td id=\"T_4795f_row1_col10\" class=\"data row1 col10\" >29.33%</td>\n",
       "      <td id=\"T_4795f_row1_col11\" class=\"data row1 col11\" >0.7209</td>\n",
       "      <td id=\"T_4795f_row1_col12\" class=\"data row1 col12\" >/kaggle/working//5min_total_bytes_setup1_replica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4795f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_4795f_row2_col0\" class=\"data row2 col0\" >15min</td>\n",
       "      <td id=\"T_4795f_row2_col1\" class=\"data row2 col1\" >request_count</td>\n",
       "      <td id=\"T_4795f_row2_col2\" class=\"data row2 col2\" >154.81</td>\n",
       "      <td id=\"T_4795f_row2_col3\" class=\"data row2 col3\" >226.28</td>\n",
       "      <td id=\"T_4795f_row2_col4\" class=\"data row2 col4\" >51201.37</td>\n",
       "      <td id=\"T_4795f_row2_col5\" class=\"data row2 col5\" >24.46%</td>\n",
       "      <td id=\"T_4795f_row2_col6\" class=\"data row2 col6\" >0.5933</td>\n",
       "      <td id=\"T_4795f_row2_col7\" class=\"data row2 col7\" >83.81</td>\n",
       "      <td id=\"T_4795f_row2_col8\" class=\"data row2 col8\" >114.77</td>\n",
       "      <td id=\"T_4795f_row2_col9\" class=\"data row2 col9\" >13171.50</td>\n",
       "      <td id=\"T_4795f_row2_col10\" class=\"data row2 col10\" >16.03%</td>\n",
       "      <td id=\"T_4795f_row2_col11\" class=\"data row2 col11\" >0.8954</td>\n",
       "      <td id=\"T_4795f_row2_col12\" class=\"data row2 col12\" >/kaggle/working//15min_request_count_setup1_replica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4795f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_4795f_row3_col0\" class=\"data row3 col0\" >15min</td>\n",
       "      <td id=\"T_4795f_row3_col1\" class=\"data row3 col1\" >total_bytes</td>\n",
       "      <td id=\"T_4795f_row3_col2\" class=\"data row3 col2\" >2491716.33</td>\n",
       "      <td id=\"T_4795f_row3_col3\" class=\"data row3 col3\" >3485437.97</td>\n",
       "      <td id=\"T_4795f_row3_col4\" class=\"data row3 col4\" >12148277865859.69</td>\n",
       "      <td id=\"T_4795f_row3_col5\" class=\"data row3 col5\" >30.77%</td>\n",
       "      <td id=\"T_4795f_row3_col6\" class=\"data row3 col6\" >0.3990</td>\n",
       "      <td id=\"T_4795f_row3_col7\" class=\"data row3 col7\" >1502758.73</td>\n",
       "      <td id=\"T_4795f_row3_col8\" class=\"data row3 col8\" >1998017.18</td>\n",
       "      <td id=\"T_4795f_row3_col9\" class=\"data row3 col9\" >3992072664394.16</td>\n",
       "      <td id=\"T_4795f_row3_col10\" class=\"data row3 col10\" >22.56%</td>\n",
       "      <td id=\"T_4795f_row3_col11\" class=\"data row3 col11\" >0.8025</td>\n",
       "      <td id=\"T_4795f_row3_col12\" class=\"data row3 col12\" >/kaggle/working//15min_total_bytes_setup1_replica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x79ccd085d610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "HYBRID IMPROVEMENTS\n",
      "======================================================================\n",
      "\n",
      "5min | request_count:\n",
      "  Hybrid MAE: 34.96\n",
      "  Improvement over Prophet: +38.8%\n",
      "  Improvement over LSTM: +0.0%\n",
      "\n",
      "5min | total_bytes:\n",
      "  Hybrid MAE: 632106.99\n",
      "  Improvement over Prophet: +31.9%\n",
      "  Improvement over LSTM: +0.0%\n",
      "\n",
      "15min | request_count:\n",
      "  Hybrid MAE: 83.81\n",
      "  Improvement over Prophet: +45.9%\n",
      "  Improvement over LSTM: +0.0%\n",
      "\n",
      "15min | total_bytes:\n",
      "  Hybrid MAE: 1502758.73\n",
      "  Improvement over Prophet: +39.7%\n",
      "  Improvement over LSTM: +0.0%\n",
      "\n",
      "======================================================================\n",
      "OVERALL STATISTICS\n",
      "======================================================================\n",
      "\n",
      "Average Hybrid Improvement:\n",
      "  vs Prophet: +39.1%\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# CELL 7: CREATE COMPREHENSIVE BENCHMARK\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERATING COMPREHENSIVE BENCHMARK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create benchmark directory\n",
    "benchmark_dir = f\"{RESULTS_BASE_DIR}/FINAL_BENCHMARK\"\n",
    "os.makedirs(benchmark_dir, exist_ok=True)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "benchmark_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save comprehensive comparison\n",
    "benchmark_file = f\"{benchmark_dir}/comprehensive_comparison.csv\"\n",
    "benchmark_df.to_csv(benchmark_file, index=False)\n",
    "print(f\"\\n‚úì Benchmark saved: {benchmark_file}\")\n",
    "\n",
    "print(\"\\nüìä BENCHMARK RESULTS:\\n\")\n",
    "display(benchmark_df.style.background_gradient(\n",
    "    cmap='RdYlGn_r', \n",
    "    subset=['prophet_mae', 'hybrid_mae', 'hybrid_rmse']\n",
    ").format({\n",
    "    'prophet_mae': '{:.2f}',\n",
    "    'prophet_rmse': '{:.2f}',\n",
    "    'prophet_mse': '{:.2f}',\n",
    "    'prophet_mape': '{:.2f}%',\n",
    "    'prophet_r2': '{:.4f}',\n",
    "    'hybrid_mae': '{:.2f}',\n",
    "    'hybrid_rmse': '{:.2f}',\n",
    "    'hybrid_mse': '{:.2f}',\n",
    "    'hybrid_mape': '{:.2f}%',\n",
    "    'hybrid_r2': '{:.4f}',\n",
    "    'training_time_sec': '{:.1f}s'\n",
    "}))\n",
    "\n",
    "# Calculate improvements\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYBRID IMPROVEMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx, row in benchmark_df.iterrows():\n",
    "    prophet_imp = ((row['prophet_mae'] - row['hybrid_mae']) / row['prophet_mae']) * 100\n",
    "    \n",
    "    print(f\"\\n{row['resolution']} | {row['target']}:\")\n",
    "    print(f\"  Hybrid MAE: {row['hybrid_mae']:.2f}\")\n",
    "    print(f\"  Improvement over Prophet: {prophet_imp:+.1f}%\")\n",
    "    print(f\"  Improvement over LSTM: {lstm_imp:+.1f}%\")\n",
    "    # print(f\"  Anomaly rate: {row['anomaly_rate']:.2f}%\")\n",
    "\n",
    "# Overall statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OVERALL STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "avg_prophet_imp = ((benchmark_df['prophet_mae'] - benchmark_df['hybrid_mae']) / benchmark_df['prophet_mae']).mean() * 100\n",
    "\n",
    "print(f\"\\nAverage Hybrid Improvement:\")\n",
    "print(f\"  vs Prophet: {avg_prophet_imp:+.1f}%\")\n",
    "# print(f\"\\nAverage Anomaly Rate: {benchmark_df['anomaly_rate'].mean():.2f}%\")\n",
    "# print(f\"Total Anomalies Detected: {benchmark_df['anomalies_detected'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:40:58.578890Z",
     "iopub.status.busy": "2026-02-02T13:40:58.578594Z",
     "iopub.status.idle": "2026-02-02T13:40:58.591362Z",
     "shell.execute_reply": "2026-02-02T13:40:58.590743Z",
     "shell.execute_reply.started": "2026-02-02T13:40:58.578865Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üéâ AUTOMATED HYBRID PIPELINE COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "üìä SUMMARY:\n",
      "  Total configurations: 4\n",
      "  Total time: 4.6 minutes\n",
      "  Average time per config: 69.2 seconds\n",
      "\n",
      "üìÅ RESULTS LOCATION:\n",
      "  Main directory: /kaggle/working/\n",
      "  Benchmark: /kaggle/working//FINAL_BENCHMARK\n",
      "\n",
      "üèÜ BEST CONFIGURATION:\n",
      "  5min | request_count\n",
      "  Hybrid MAE: 34.96\n",
      "  Improvement: +39.1% vs Prophet, -0.0% vs LSTM\n",
      "\n",
      "üìà TOP 3 PERFORMERS (by Hybrid MAE):\n",
      "  5min  | request_count   | MAE:  34.96 | RMSE:  46.99\n",
      "  15min | request_count   | MAE:  83.81 | RMSE: 114.77\n",
      "  5min  | total_bytes     | MAE: 632106.99 | RMSE: 847233.17\n",
      "\n",
      "üí° NEXT STEPS:\n",
      "  1. Review final_report.txt in /kaggle/working//FINAL_BENCHMARK\n",
      "  2. Check benchmark_visualizations.png\n",
      "  3. Analyze anomalies.csv for each configuration\n",
      "  4. Deploy best hybrid model to production\n",
      "  5. Set up monitoring for anomaly alerts\n",
      "  6. Implement retraining pipeline (weekly Prophet, daily LSTM)\n",
      "\n",
      "================================================================================\n",
      "All results have been saved to Google Drive!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# CELL 10: SUMMARY & NEXT STEPS\n",
    "# ===========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ AUTOMATED HYBRID PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä SUMMARY:\")\n",
    "print(f\"  Total configurations: {len(all_results)}\")\n",
    "print(f\"  Total time: {total_elapsed/60:.1f} minutes\")\n",
    "print(f\"  Average time per config: {total_elapsed/len(all_results):.1f} seconds\")\n",
    "\n",
    "print(f\"\\nüìÅ RESULTS LOCATION:\")\n",
    "print(f\"  Main directory: {RESULTS_BASE_DIR}\")\n",
    "print(f\"  Benchmark: {benchmark_dir}\")\n",
    "\n",
    "print(f\"\\nüèÜ BEST CONFIGURATION:\")\n",
    "print(f\"  {best['resolution']} | {best['target']}\")\n",
    "print(f\"  Hybrid MAE: {best['hybrid_mae']:.2f}\")\n",
    "print(f\"  Improvement: {avg_prophet_imp:+.1f}% vs Prophet, {avg_lstm_imp:+.1f}% vs LSTM\")\n",
    "\n",
    "# print(f\"\\nüîç ANOMALY DETECTION:\")\n",
    "# print(f\"  Total anomalies: {benchmark_df['anomalies_detected'].sum():,}\")\n",
    "# print(f\"  Average rate: {benchmark_df['anomaly_rate'].mean():.2f}%\")\n",
    "\n",
    "print(f\"\\nüìà TOP 3 PERFORMERS (by Hybrid MAE):\")\n",
    "top_3 = benchmark_df.nsmallest(3, 'hybrid_mae')[['resolution', 'target', 'hybrid_mae', 'hybrid_rmse']]\n",
    "for idx, row in top_3.iterrows():\n",
    "    print(f\"  {row['resolution']:5s} | {row['target']:15s} | MAE: {row['hybrid_mae']:6.2f} | RMSE: {row['hybrid_rmse']:6.2f}\")\n",
    "\n",
    "print(f\"\\nüí° NEXT STEPS:\")\n",
    "print(f\"  1. Review final_report.txt in {benchmark_dir}\")\n",
    "print(f\"  2. Check benchmark_visualizations.png\")\n",
    "print(f\"  3. Analyze anomalies.csv for each configuration\")\n",
    "print(f\"  4. Deploy best hybrid model to production\")\n",
    "print(f\"  5. Set up monitoring for anomaly alerts\")\n",
    "print(f\"  6. Implement retraining pipeline (weekly Prophet, daily LSTM)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"All results have been saved to Google Drive!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T13:41:09.641934Z",
     "iopub.status.busy": "2026-02-02T13:41:09.641187Z",
     "iopub.status.idle": "2026-02-02T13:41:09.941594Z",
     "shell.execute_reply": "2026-02-02T13:41:09.940861Z",
     "shell.execute_reply.started": "2026-02-02T13:41:09.641905Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: 5min_request_count_raw_no_log/ (stored 0%)\n",
      "updating: 5min_request_count_raw_no_log/prophet_model/ (stored 0%)\n",
      "updating: 15min_request_count_raw_no_log/ (stored 0%)\n",
      "updating: 15min_request_count_raw_no_log/prophet_model/ (stored 0%)\n",
      "updating: 5min_request_count_final/ (stored 0%)\n",
      "updating: 5min_request_count_final/prophet_model/ (stored 0%)\n",
      "updating: .virtual_documents/ (stored 0%)\n",
      "updating: .virtual_documents/__notebook_source__.ipynb (deflated 80%)\n",
      "updating: 5min_total_bytes_raw_no_log/ (stored 0%)\n",
      "updating: 5min_total_bytes_raw_no_log/prophet_model/ (stored 0%)\n",
      "updating: 15min_total_bytes_raw_no_log/ (stored 0%)\n",
      "updating: 15min_total_bytes_raw_no_log/prophet_model/ (stored 0%)\n",
      "updating: FINAL_BENCHMARK/ (stored 0%)\n",
      "updating: FINAL_BENCHMARK/final_report.txt (deflated 67%)\n",
      "updating: FINAL_BENCHMARK/comprehensive_comparison.csv (deflated 50%)\n",
      "updating: FINAL_BENCHMARK/benchmark_visualizations.png (deflated 16%)\n",
      "updating: 5min_total_bytes_final/ (stored 0%)\n",
      "updating: 5min_total_bytes_final/prophet_model/ (stored 0%)\n",
      "  adding: 15min_request_count_setup1_replica/ (stored 0%)\n",
      "  adding: 15min_request_count_setup1_replica/prophet_model/ (stored 0%)\n",
      "  adding: 15min_request_count_setup1_replica/hybrid_predictions.csv (deflated 59%)\n",
      "  adding: 15min_request_count_setup1_revival/ (stored 0%)\n",
      "  adding: 15min_request_count_setup1_revival/prophet_model/ (stored 0%)\n",
      "  adding: 15min_request_count_setup1_revival/hybrid_predictions.csv (deflated 59%)\n",
      "  adding: 15min_request_count_upgrade/ (stored 0%)\n",
      "  adding: 15min_request_count_upgrade/prophet_model/ (stored 0%)\n",
      "  adding: 15min_request_count_upgrade/hybrid_predictions.csv (deflated 56%)\n",
      "  adding: 5min_request_count_setup1_replica/ (stored 0%)\n",
      "  adding: 5min_request_count_setup1_replica/prophet_model/ (stored 0%)\n",
      "  adding: 5min_request_count_setup1_replica/hybrid_predictions.csv (deflated 61%)\n",
      "  adding: 15min_total_bytes_setup1_revival/ (stored 0%)\n",
      "  adding: 15min_total_bytes_setup1_revival/prophet_model/ (stored 0%)\n",
      "  adding: 15min_total_bytes_setup1_revival/hybrid_predictions.csv (deflated 60%)\n",
      "  adding: 5min_request_count_rolling/ (stored 0%)\n",
      "  adding: 5min_request_count_rolling/prophet_model/ (stored 0%)\n",
      "  adding: 5min_request_count_upgrade/ (stored 0%)\n",
      "  adding: 5min_request_count_upgrade/prophet_model/ (stored 0%)\n",
      "  adding: 5min_request_count_upgrade/hybrid_predictions.csv (deflated 58%)\n",
      "  adding: 5min_total_bytes_setup1_revival/ (stored 0%)\n",
      "  adding: 5min_total_bytes_setup1_revival/prophet_model/ (stored 0%)\n",
      "  adding: 5min_total_bytes_setup1_revival/hybrid_predictions.csv (deflated 60%)\n",
      "  adding: 5min_request_count_setup1_revival/ (stored 0%)\n",
      "  adding: 5min_request_count_setup1_revival/prophet_model/ (stored 0%)\n",
      "  adding: 5min_request_count_setup1_revival/hybrid_predictions.csv (deflated 61%)\n",
      "  adding: 15min_request_count_rolling/ (stored 0%)\n",
      "  adding: 15min_request_count_rolling/prophet_model/ (stored 0%)\n",
      "  adding: 15min_total_bytes_setup1_replica/ (stored 0%)\n",
      "  adding: 15min_total_bytes_setup1_replica/prophet_model/ (stored 0%)\n",
      "  adding: 15min_total_bytes_setup1_replica/hybrid_predictions.csv (deflated 60%)\n",
      "  adding: 15min_total_bytes_upgrade/ (stored 0%)\n",
      "  adding: 15min_total_bytes_upgrade/prophet_model/ (stored 0%)\n",
      "  adding: 15min_total_bytes_upgrade/hybrid_predictions.csv (deflated 56%)\n",
      "  adding: 5min_total_bytes_rolling/ (stored 0%)\n",
      "  adding: 5min_total_bytes_rolling/prophet_model/ (stored 0%)\n",
      "  adding: 5min_total_bytes_upgrade/ (stored 0%)\n",
      "  adding: 5min_total_bytes_upgrade/prophet_model/ (stored 0%)\n",
      "  adding: 5min_total_bytes_upgrade/hybrid_predictions.csv (deflated 57%)\n",
      "  adding: 5min_total_bytes_setup1_replica/ (stored 0%)\n",
      "  adding: 5min_total_bytes_setup1_replica/prophet_model/ (stored 0%)\n",
      "  adding: 5min_total_bytes_setup1_replica/hybrid_predictions.csv (deflated 60%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='all_output.zip' target='_blank'>all_output.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/all_output.zip"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N√©n to√†n b·ªô file trong th∆∞ m·ª•c hi·ªán t·∫°i (recursive)\n",
    "!zip -r all_output.zip .\n",
    "\n",
    "# T·∫°o link t·∫£i\n",
    "from IPython.display import FileLink\n",
    "FileLink('all_output.zip')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9396610,
     "sourceId": 14707778,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
